{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "# from evaluate import load as load_metric\n",
    "from miditok import REMI\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from torch import Tensor, argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_utils import set_seed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Pobranie ścieżki do folderu głównego projektu (dostosuj, jeśli to konieczne)\n",
    "project_path = os.path.abspath(\"../minGPT_\")\n",
    "# Dodanie tej ścieżki do `sys.path`\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "# Teraz powinno działać\n",
    "from minGPT_.projects.midi.midi import MidiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'midi_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split MIDI paths in train/valid/test sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m total_num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmidi_paths\u001b[49m)\n\u001b[0;32m      3\u001b[0m num_files_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(total_num_files \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.15\u001b[39m)\n\u001b[0;32m      4\u001b[0m num_files_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(total_num_files \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.15\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'midi_paths' is not defined"
     ]
    }
   ],
   "source": [
    "# Split MIDI paths in train/valid/test sets\n",
    "total_num_files = len(midi_paths)\n",
    "num_files_valid = round(total_num_files * 0.15)\n",
    "num_files_test = round(total_num_files * 0.15)\n",
    "shuffle(midi_paths)\n",
    "midi_paths_valid = midi_paths[:num_files_valid]\n",
    "midi_paths_test = midi_paths[num_files_valid : num_files_valid + num_files_test]\n",
    "midi_paths_train = midi_paths[num_files_valid + num_files_test :]\n",
    "\n",
    "# # Chunk MIDIs and perform data augmentation on each subset independently\n",
    "for files_paths, subset_name in (\n",
    "    (midi_paths_train, \"train\"),\n",
    "    (midi_paths_valid, \"valid\"),\n",
    "    (midi_paths_test, \"test\"),\n",
    "):\n",
    "\n",
    "    # Split the MIDIs into chunks of sizes approximately about 1024 tokens\n",
    "    subset_chunks_dir = Path(f\"filtered_midi/aug/Maestro_{subset_name}\")\n",
    "    split_files_for_training(\n",
    "        files_paths=files_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=subset_chunks_dir,\n",
    "        max_seq_len=1024,\n",
    "        num_overlap_bars=2,\n",
    "    )\n",
    "\n",
    "# Perform data augmentation\n",
    "augment_dataset(\n",
    "    subset_chunks_dir,\n",
    "    pitch_offsets=[-12, 12],\n",
    "    velocity_offsets=[-4, 4],\n",
    "    duration_offsets=[-0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('tokenizer_filtered.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = Path(\"tokenizer_filtered.json\")\n",
    "tokenizer = REMI(params=tokenizer_path)\n",
    "\n",
    "pad_token = tokenizer[\"PAD_None\"]\n",
    "tokenizer_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_midi\\Maestro_train\\Blues\\(Sittin On) The Dock Of The Bay_0.mid\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"filtered_midi\"\n",
    "\n",
    "# Create Dataset and Collator for training\n",
    "\n",
    "midi_paths_train = list(Path(f\"{dir_name}/Maestro_train\").glob(\"**/*.mid\")) + list(\n",
    "    Path(f\"{dir_name}/Maestro_train\").glob(\"**/*.midi\")\n",
    ")\n",
    "\n",
    "midi_paths_valid = list(Path(f\"{dir_name}/Maestro_valid\").glob(\"**/*.mid\")) + list(\n",
    "    Path(f\"{dir_name}/Maestro_valid\").glob(\"**/*.midi\")\n",
    ")\n",
    "\n",
    "midi_paths_test = list(Path(f\"{dir_name}/Maestro_test\").glob(\"**/*.mid\")) + list(\n",
    "    Path(f\"{dir_name}/Maestro_test\").glob(\"**/*.midi\")\n",
    ")\n",
    "\n",
    "kwargs_dataset = {\n",
    "    \"max_seq_len\": 1024,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"bos_token_id\": tokenizer[\"BOS_None\"],\n",
    "    \"eos_token_id\": tokenizer[\"EOS_None\"],\n",
    "}\n",
    "\n",
    "dataset_train = DatasetMIDI(midi_paths_train, **kwargs_dataset)\n",
    "\n",
    "dataset_valid = DatasetMIDI(midi_paths_valid, **kwargs_dataset)\n",
    "\n",
    "dataset_test = DatasetMIDI(midi_paths_test, **kwargs_dataset)\n",
    "print(midi_paths_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_config = {\"max_seq_len\": 1024, \"pad_token_id\": pad_token, \"pred_num\": 1}\n",
    "\n",
    "train_dataset = MidiDataset(dataset_train, **tran_config)\n",
    "valid_dataset = MidiDataset(dataset_valid, **tran_config)\n",
    "test_dataset = MidiDataset(dataset_test, **tran_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8.63M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = \"gpt-nano\"\n",
    "# model_config.model_type = \"gpt-micro\"\n",
    "model_config.model_type = \"gpt-mini\"\n",
    "\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "model_config.block_size = 1024\n",
    "\n",
    "\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6 192\n"
     ]
    }
   ],
   "source": [
    "print(model_config.n_head, model_config.n_layer, model_config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "\n",
    "train_config.learning_rate = (\n",
    "    2e-5  # the model we're using is so small that we can go a bit faster\n",
    ")\n",
    "train_config.batch_size = 4\n",
    "train_config.max_iters = 10000\n",
    "train_config.num_workers = 4\n",
    "train_config.weight_decay = 0.01\n",
    "train_config.lr_decay = True\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 10.35037\n",
      "iter_dt 140.00ms; iter 100: train loss 7.24491\n",
      "iter_dt 149.28ms; iter 200: train loss 7.09906\n",
      "iter_dt 150.49ms; iter 300: train loss 6.72025\n",
      "iter_dt 145.54ms; iter 400: train loss 7.65948\n",
      "iter_dt 150.51ms; iter 500: train loss 6.10308\n",
      "iter_dt 155.29ms; iter 600: train loss 4.81339\n",
      "iter_dt 155.49ms; iter 700: train loss 4.56344\n",
      "iter_dt 152.79ms; iter 800: train loss 3.59995\n",
      "iter_dt 145.43ms; iter 900: train loss 3.75859\n",
      "iter_dt 152.75ms; iter 1000: train loss 3.70889\n",
      "iter_dt 156.75ms; iter 1100: train loss 5.32608\n",
      "iter_dt 157.64ms; iter 1200: train loss 3.58974\n",
      "iter_dt 145.54ms; iter 1300: train loss 3.13748\n",
      "iter_dt 153.42ms; iter 1400: train loss 4.14285\n",
      "iter_dt 157.55ms; iter 1500: train loss 3.27124\n",
      "iter_dt 162.90ms; iter 1600: train loss 3.22933\n",
      "iter_dt 153.73ms; iter 1700: train loss 3.98922\n",
      "iter_dt 156.08ms; iter 1800: train loss 2.12678\n",
      "iter_dt 152.57ms; iter 1900: train loss 2.25691\n",
      "iter_dt 150.02ms; iter 2000: train loss 2.64809\n",
      "iter_dt 146.92ms; iter 2100: train loss 4.50154\n",
      "iter_dt 151.45ms; iter 2200: train loss 2.18614\n",
      "iter_dt 149.21ms; iter 2300: train loss 1.80985\n",
      "iter_dt 153.78ms; iter 2400: train loss 3.39404\n",
      "iter_dt 147.31ms; iter 2500: train loss 3.07293\n",
      "iter_dt 163.30ms; iter 2600: train loss 3.03807\n",
      "iter_dt 163.92ms; iter 2700: train loss 4.22587\n",
      "iter_dt 145.28ms; iter 2800: train loss 3.72803\n",
      "iter_dt 154.39ms; iter 2900: train loss 3.53514\n",
      "iter_dt 147.61ms; iter 3000: train loss 4.01526\n",
      "iter_dt 149.06ms; iter 3100: train loss 2.93237\n",
      "iter_dt 152.74ms; iter 3200: train loss 2.77023\n",
      "iter_dt 150.50ms; iter 3300: train loss 5.41864\n",
      "iter_dt 163.78ms; iter 3400: train loss 4.42258\n",
      "iter_dt 149.54ms; iter 3500: train loss 2.39425\n",
      "iter_dt 146.70ms; iter 3600: train loss 2.34536\n",
      "iter_dt 148.26ms; iter 3700: train loss 5.43861\n",
      "iter_dt 147.11ms; iter 3800: train loss 2.27529\n",
      "iter_dt 150.00ms; iter 3900: train loss 2.60311\n",
      "iter_dt 151.60ms; iter 4000: train loss 2.73683\n",
      "iter_dt 148.22ms; iter 4100: train loss 1.87059\n",
      "iter_dt 151.81ms; iter 4200: train loss 2.47047\n",
      "iter_dt 152.91ms; iter 4300: train loss 3.40429\n",
      "iter_dt 149.56ms; iter 4400: train loss 3.90106\n",
      "iter_dt 145.50ms; iter 4500: train loss 2.27937\n",
      "iter_dt 152.24ms; iter 4600: train loss 1.59090\n",
      "iter_dt 149.65ms; iter 4700: train loss 5.68931\n",
      "iter_dt 158.85ms; iter 4800: train loss 4.25536\n",
      "iter_dt 151.41ms; iter 4900: train loss 4.03582\n",
      "iter_dt 153.24ms; iter 5000: train loss 2.77339\n",
      "iter_dt 169.81ms; iter 5100: train loss 2.59768\n",
      "iter_dt 143.63ms; iter 5200: train loss 3.77809\n",
      "iter_dt 148.31ms; iter 5300: train loss 2.14363\n",
      "iter_dt 155.70ms; iter 5400: train loss 2.27085\n",
      "iter_dt 151.67ms; iter 5500: train loss 1.98338\n",
      "iter_dt 151.25ms; iter 5600: train loss 4.30241\n",
      "iter_dt 151.79ms; iter 5700: train loss 2.68880\n",
      "iter_dt 148.90ms; iter 5800: train loss 3.64832\n",
      "iter_dt 150.58ms; iter 5900: train loss 4.39921\n",
      "iter_dt 146.98ms; iter 6000: train loss 3.36120\n",
      "iter_dt 147.46ms; iter 6100: train loss 4.07957\n",
      "iter_dt 147.24ms; iter 6200: train loss 4.06632\n",
      "iter_dt 147.38ms; iter 6300: train loss 3.85745\n",
      "iter_dt 147.73ms; iter 6400: train loss 2.38912\n",
      "iter_dt 146.46ms; iter 6500: train loss 2.43641\n",
      "iter_dt 147.66ms; iter 6600: train loss 2.52300\n",
      "iter_dt 162.53ms; iter 6700: train loss 4.48147\n",
      "iter_dt 151.48ms; iter 6800: train loss 3.80980\n",
      "iter_dt 156.71ms; iter 6900: train loss 2.78384\n",
      "iter_dt 148.37ms; iter 7000: train loss 4.29979\n",
      "iter_dt 147.48ms; iter 7100: train loss 3.53480\n",
      "iter_dt 148.35ms; iter 7200: train loss 2.54317\n",
      "iter_dt 147.42ms; iter 7300: train loss 4.77641\n",
      "iter_dt 148.29ms; iter 7400: train loss 3.24526\n",
      "iter_dt 142.26ms; iter 7500: train loss 1.56516\n",
      "iter_dt 145.21ms; iter 7600: train loss 3.73197\n",
      "iter_dt 144.14ms; iter 7700: train loss 3.63207\n",
      "iter_dt 149.73ms; iter 7800: train loss 2.37267\n",
      "iter_dt 152.43ms; iter 7900: train loss 4.33311\n",
      "iter_dt 150.18ms; iter 8000: train loss 4.90549\n",
      "iter_dt 153.76ms; iter 8100: train loss 2.75218\n",
      "iter_dt 148.42ms; iter 8200: train loss 4.26586\n",
      "iter_dt 152.63ms; iter 8300: train loss 3.43205\n",
      "iter_dt 149.47ms; iter 8400: train loss 4.06629\n",
      "iter_dt 142.20ms; iter 8500: train loss 2.41770\n",
      "iter_dt 150.54ms; iter 8600: train loss 3.01236\n",
      "iter_dt 150.42ms; iter 8700: train loss 4.01724\n",
      "iter_dt 148.47ms; iter 8800: train loss 3.26789\n",
      "iter_dt 148.30ms; iter 8900: train loss 2.61045\n",
      "iter_dt 148.64ms; iter 9000: train loss 4.15116\n",
      "iter_dt 158.29ms; iter 9100: train loss 2.11793\n",
      "iter_dt 144.00ms; iter 9200: train loss 2.60572\n",
      "iter_dt 150.81ms; iter 9300: train loss 4.53813\n",
      "iter_dt 140.28ms; iter 9400: train loss 5.44872\n",
      "iter_dt 149.68ms; iter 9500: train loss 3.33285\n",
      "iter_dt 161.75ms; iter 9600: train loss 1.69832\n",
      "iter_dt 149.05ms; iter 9700: train loss 3.14238\n",
      "iter_dt 144.14ms; iter 9800: train loss 3.16469\n",
      "iter_dt 151.68ms; iter 9900: train loss 3.20233\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "losses = []\n",
    "def model_info_to_json(model_name):\n",
    "    (path := Path(\"eval\")).mkdir(exist_ok=True)\n",
    "    \n",
    "    path_json_l = f\"{path}/losses.json\"\n",
    "    path_json_c = f\"{path}/config.json\"\n",
    "   \n",
    "    json.dump(losses, path_json_l)\n",
    "    json.dump(train_config, open(path_json_c, \"w\"))\n",
    "    \n",
    "\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "        losses.append(trainer.loss.item())\n",
    "    \n",
    "trainer.set_callback(\"on_batch_end\", batch_end_callback)\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_info_to_json(model_name):\n",
    "    (path := Path(\"eval\")).mkdir(exist_ok=True)\n",
    "\n",
    "    path_json_l = f\"{path}/losses_6.json\"\n",
    "    with open(path_json_l, \"w\") as f:\n",
    "        json.dump(losses, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_to_json(\"gpt-mini\")\n",
    "torch.save(model.state_dict(), \"gpt_mini.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- @format -->\n",
    "\n",
    "### Model GPT micro\n",
    "\n",
    "śr czas na iteracje 80ms\n",
    "łączna ilość iteracji - 20 000\n",
    "loss~6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gen_results_path := Path(\"gen_res\")).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8.63M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = \"gpt-nano\"\n",
    "# model_config.model_type = \"gpt-micro\"\n",
    "model_config.model_type = \"gpt-mini\"\n",
    "\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "model_config.block_size = 1024\n",
    "\n",
    "\n",
    "model = GPT(model_config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_5524\\1237603414.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path_to_model))\n"
     ]
    }
   ],
   "source": [
    "path_to_model = \"gpt_mini.pth\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing model / Generating results:   0%|          | 10/3114 [05:08<26:35:15, 30.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader_test, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model / Generating results\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.87\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Saves the generated music, as MIDI files and tokens (json)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prompt, continuation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], res):\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\mikol\\desktop\\studia\\5_sem\\language_models\\mingpt\\mingpt\\model.py:301\u001b[0m, in \u001b[0;36mGPT.generate\u001b[1;34m(self, idx, max_new_tokens, temperature, do_sample, top_k)\u001b[0m\n\u001b[0;32m    299\u001b[0m     logits[logits \u001b[38;5;241m<\u001b[39m v[:, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# apply softmax to convert logits to (normalized) probabilities\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# either sample from the distribution or take the most likely element\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2103\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2099\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 2103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\n\u001b[0;32m   2104\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m   2105\u001b[0m     dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2106\u001b[0m     _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   2107\u001b[0m     dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \n\u001b[0;32m   2111\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \n\u001b[0;32m   2132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "collator = DataCollator(tokenizer[\"PAD_None\"], copy_inputs_as_labels=True)\n",
    "collator.pad_on_left = True\n",
    "collator.eos_token = None\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=4, collate_fn=collator)\n",
    "\n",
    "count = 0\n",
    "for batch in tqdm(dataloader_test, desc=\"Testing model / Generating results\"):\n",
    "    res = model.generate(\n",
    "        idx=batch[\"input_ids\"].to(\"cuda\"), max_new_tokens=800, do_sample=True, top_k=50, temperature=0.87\n",
    "    )\n",
    "\n",
    "    # Saves the generated music, as MIDI files and tokens (json)\n",
    "    for prompt, continuation in zip(batch[\"input_ids\"], res):\n",
    "        generated = continuation[len(prompt) :]\n",
    "        midi = tokenizer.decode([deepcopy(generated.tolist())])\n",
    "\n",
    "        tokens = [generated, prompt, continuation]\n",
    "        tokens = [seq.tolist() for seq in tokens]\n",
    "\n",
    "        for tok_seq in tokens[1:]:\n",
    "            _midi = tokenizer.decode([deepcopy(tok_seq)])\n",
    "            midi.tracks.append(_midi.tracks[0])\n",
    "\n",
    "        midi_name = [f\"Continuation of original sample ({len(generated)} tokens)\", f\"Original sample ({len(prompt)} tokens)\", f\"Original sample and continuation\"]\n",
    "\n",
    "        for i in range(min(len(midi.tracks), len(midi_name))):\n",
    "            midi.tracks[i].name = midi_name[i]\n",
    "        midi.dump_midi(gen_results_path / f\"{count}.mid\")\n",
    "        \n",
    "\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
