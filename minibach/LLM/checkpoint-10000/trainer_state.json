{
  "best_metric": 6.127485752105713,
  "best_model_checkpoint": "runs\\checkpoint-10000",
  "epoch": 10.314595152140278,
  "eval_steps": 500,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020629190304280558,
      "grad_norm": 3.502920389175415,
      "learning_rate": 6.666666666666667e-07,
      "loss": 10.3535,
      "step": 20
    },
    {
      "epoch": 0.041258380608561115,
      "grad_norm": 3.690094470977783,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 10.3514,
      "step": 40
    },
    {
      "epoch": 0.06188757091284167,
      "grad_norm": 3.170349597930908,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 10.3449,
      "step": 60
    },
    {
      "epoch": 0.08251676121712223,
      "grad_norm": 4.288968086242676,
      "learning_rate": 2.666666666666667e-06,
      "loss": 10.343,
      "step": 80
    },
    {
      "epoch": 0.10314595152140278,
      "grad_norm": 5.5196990966796875,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 10.3298,
      "step": 100
    },
    {
      "epoch": 0.12377514182568335,
      "grad_norm": 3.4370429515838623,
      "learning_rate": 4.000000000000001e-06,
      "loss": 10.3197,
      "step": 120
    },
    {
      "epoch": 0.1444043321299639,
      "grad_norm": 3.075251817703247,
      "learning_rate": 4.666666666666667e-06,
      "loss": 10.3013,
      "step": 140
    },
    {
      "epoch": 0.16503352243424446,
      "grad_norm": 3.579927444458008,
      "learning_rate": 5.333333333333334e-06,
      "loss": 10.2794,
      "step": 160
    },
    {
      "epoch": 0.18566271273852503,
      "grad_norm": 3.14776873588562,
      "learning_rate": 6e-06,
      "loss": 10.2583,
      "step": 180
    },
    {
      "epoch": 0.20629190304280556,
      "grad_norm": 2.2001264095306396,
      "learning_rate": 6.666666666666667e-06,
      "loss": 10.2406,
      "step": 200
    },
    {
      "epoch": 0.22692109334708613,
      "grad_norm": 2.3436098098754883,
      "learning_rate": 7.333333333333334e-06,
      "loss": 10.2027,
      "step": 220
    },
    {
      "epoch": 0.2475502836513667,
      "grad_norm": 2.0992844104766846,
      "learning_rate": 8.000000000000001e-06,
      "loss": 10.1893,
      "step": 240
    },
    {
      "epoch": 0.26817947395564723,
      "grad_norm": 2.484387159347534,
      "learning_rate": 8.666666666666668e-06,
      "loss": 10.1745,
      "step": 260
    },
    {
      "epoch": 0.2888086642599278,
      "grad_norm": 1.8597404956817627,
      "learning_rate": 9.333333333333334e-06,
      "loss": 10.153,
      "step": 280
    },
    {
      "epoch": 0.30943785456420836,
      "grad_norm": 2.9294686317443848,
      "learning_rate": 1e-05,
      "loss": 10.1189,
      "step": 300
    },
    {
      "epoch": 0.3300670448684889,
      "grad_norm": 2.6751668453216553,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 10.0939,
      "step": 320
    },
    {
      "epoch": 0.3506962351727695,
      "grad_norm": 1.6180880069732666,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 10.0566,
      "step": 340
    },
    {
      "epoch": 0.37132542547705005,
      "grad_norm": 2.26814341545105,
      "learning_rate": 1.2e-05,
      "loss": 10.0402,
      "step": 360
    },
    {
      "epoch": 0.39195461578133056,
      "grad_norm": 2.691167116165161,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 10.0075,
      "step": 380
    },
    {
      "epoch": 0.4125838060856111,
      "grad_norm": 1.5968211889266968,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 9.9789,
      "step": 400
    },
    {
      "epoch": 0.4332129963898917,
      "grad_norm": 1.8501110076904297,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 9.9361,
      "step": 420
    },
    {
      "epoch": 0.45384218669417226,
      "grad_norm": 2.6214654445648193,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 9.9177,
      "step": 440
    },
    {
      "epoch": 0.4744713769984528,
      "grad_norm": 1.8252995014190674,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 9.8888,
      "step": 460
    },
    {
      "epoch": 0.4951005673027334,
      "grad_norm": 1.3554484844207764,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 9.8646,
      "step": 480
    },
    {
      "epoch": 0.5157297576070139,
      "grad_norm": 1.780860424041748,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 9.8253,
      "step": 500
    },
    {
      "epoch": 0.5157297576070139,
      "eval_accuracy": 0.011151869083572814,
      "eval_loss": 9.793014526367188,
      "eval_runtime": 72.2748,
      "eval_samples_per_second": 22.982,
      "eval_steps_per_second": 5.756,
      "step": 500
    },
    {
      "epoch": 0.5363589479112945,
      "grad_norm": 1.2812843322753906,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 9.7841,
      "step": 520
    },
    {
      "epoch": 0.556988138215575,
      "grad_norm": 1.885626196861267,
      "learning_rate": 1.8e-05,
      "loss": 9.764,
      "step": 540
    },
    {
      "epoch": 0.5776173285198556,
      "grad_norm": 1.4813787937164307,
      "learning_rate": 1.866666666666667e-05,
      "loss": 9.721,
      "step": 560
    },
    {
      "epoch": 0.5982465188241362,
      "grad_norm": 2.9949159622192383,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 9.6757,
      "step": 580
    },
    {
      "epoch": 0.6188757091284167,
      "grad_norm": 2.3166728019714355,
      "learning_rate": 2e-05,
      "loss": 9.6295,
      "step": 600
    },
    {
      "epoch": 0.6395048994326973,
      "grad_norm": 2.1352386474609375,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 9.6198,
      "step": 620
    },
    {
      "epoch": 0.6601340897369778,
      "grad_norm": 2.036715507507324,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 9.5798,
      "step": 640
    },
    {
      "epoch": 0.6807632800412584,
      "grad_norm": 2.12986421585083,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 9.5893,
      "step": 660
    },
    {
      "epoch": 0.701392470345539,
      "grad_norm": 2.7851486206054688,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 9.5468,
      "step": 680
    },
    {
      "epoch": 0.7220216606498195,
      "grad_norm": 1.5113742351531982,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 9.4779,
      "step": 700
    },
    {
      "epoch": 0.7426508509541001,
      "grad_norm": 1.7294303178787231,
      "learning_rate": 2.4e-05,
      "loss": 9.4741,
      "step": 720
    },
    {
      "epoch": 0.7632800412583806,
      "grad_norm": 1.541387677192688,
      "learning_rate": 2.466666666666667e-05,
      "loss": 9.427,
      "step": 740
    },
    {
      "epoch": 0.7839092315626611,
      "grad_norm": 1.3517889976501465,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 9.3996,
      "step": 760
    },
    {
      "epoch": 0.8045384218669417,
      "grad_norm": 1.1396785974502563,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 9.3926,
      "step": 780
    },
    {
      "epoch": 0.8251676121712223,
      "grad_norm": 1.818920373916626,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 9.3227,
      "step": 800
    },
    {
      "epoch": 0.8457968024755028,
      "grad_norm": 1.297271728515625,
      "learning_rate": 2.733333333333333e-05,
      "loss": 9.3275,
      "step": 820
    },
    {
      "epoch": 0.8664259927797834,
      "grad_norm": 2.187814950942993,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 9.3033,
      "step": 840
    },
    {
      "epoch": 0.887055183084064,
      "grad_norm": 1.563807725906372,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 9.2706,
      "step": 860
    },
    {
      "epoch": 0.9076843733883445,
      "grad_norm": 1.528007984161377,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 9.3051,
      "step": 880
    },
    {
      "epoch": 0.9283135636926251,
      "grad_norm": 1.6028461456298828,
      "learning_rate": 3e-05,
      "loss": 9.2423,
      "step": 900
    },
    {
      "epoch": 0.9489427539969056,
      "grad_norm": 1.462615966796875,
      "learning_rate": 3.066666666666667e-05,
      "loss": 9.2042,
      "step": 920
    },
    {
      "epoch": 0.9695719443011862,
      "grad_norm": 1.8102599382400513,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 9.1917,
      "step": 940
    },
    {
      "epoch": 0.9902011346054668,
      "grad_norm": 1.4082871675491333,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 9.1785,
      "step": 960
    },
    {
      "epoch": 1.0108303249097472,
      "grad_norm": 1.6559299230575562,
      "learning_rate": 3.266666666666667e-05,
      "loss": 9.1903,
      "step": 980
    },
    {
      "epoch": 1.0314595152140278,
      "grad_norm": 1.8139355182647705,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 9.1301,
      "step": 1000
    },
    {
      "epoch": 1.0314595152140278,
      "eval_accuracy": 0.0075123979828723595,
      "eval_loss": 9.15184211730957,
      "eval_runtime": 70.1841,
      "eval_samples_per_second": 23.666,
      "eval_steps_per_second": 5.927,
      "step": 1000
    },
    {
      "epoch": 1.0520887055183084,
      "grad_norm": 1.189153790473938,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 9.1433,
      "step": 1020
    },
    {
      "epoch": 1.072717895822589,
      "grad_norm": 1.1279064416885376,
      "learning_rate": 3.466666666666667e-05,
      "loss": 9.1579,
      "step": 1040
    },
    {
      "epoch": 1.0933470861268695,
      "grad_norm": 1.834486484527588,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 9.1234,
      "step": 1060
    },
    {
      "epoch": 1.11397627643115,
      "grad_norm": 3.7583563327789307,
      "learning_rate": 3.6e-05,
      "loss": 9.0892,
      "step": 1080
    },
    {
      "epoch": 1.1346054667354306,
      "grad_norm": 1.1715478897094727,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 9.1338,
      "step": 1100
    },
    {
      "epoch": 1.1552346570397112,
      "grad_norm": 1.7452090978622437,
      "learning_rate": 3.733333333333334e-05,
      "loss": 9.0739,
      "step": 1120
    },
    {
      "epoch": 1.1758638473439917,
      "grad_norm": 3.1441586017608643,
      "learning_rate": 3.8e-05,
      "loss": 9.0653,
      "step": 1140
    },
    {
      "epoch": 1.1964930376482723,
      "grad_norm": 2.3752079010009766,
      "learning_rate": 3.866666666666667e-05,
      "loss": 9.0074,
      "step": 1160
    },
    {
      "epoch": 1.2171222279525529,
      "grad_norm": 1.6778877973556519,
      "learning_rate": 3.933333333333333e-05,
      "loss": 9.0667,
      "step": 1180
    },
    {
      "epoch": 1.2377514182568334,
      "grad_norm": 1.765305757522583,
      "learning_rate": 4e-05,
      "loss": 9.0137,
      "step": 1200
    },
    {
      "epoch": 1.258380608561114,
      "grad_norm": 1.30936861038208,
      "learning_rate": 4.066666666666667e-05,
      "loss": 9.0484,
      "step": 1220
    },
    {
      "epoch": 1.2790097988653946,
      "grad_norm": 1.7590855360031128,
      "learning_rate": 4.133333333333333e-05,
      "loss": 9.0542,
      "step": 1240
    },
    {
      "epoch": 1.2996389891696751,
      "grad_norm": 1.5550150871276855,
      "learning_rate": 4.2e-05,
      "loss": 9.0662,
      "step": 1260
    },
    {
      "epoch": 1.3202681794739557,
      "grad_norm": 1.6075440645217896,
      "learning_rate": 4.266666666666667e-05,
      "loss": 9.0229,
      "step": 1280
    },
    {
      "epoch": 1.3408973697782363,
      "grad_norm": 1.7862969636917114,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 9.053,
      "step": 1300
    },
    {
      "epoch": 1.3615265600825168,
      "grad_norm": 1.7159018516540527,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 9.0555,
      "step": 1320
    },
    {
      "epoch": 1.3821557503867974,
      "grad_norm": 1.4974784851074219,
      "learning_rate": 4.466666666666667e-05,
      "loss": 9.0344,
      "step": 1340
    },
    {
      "epoch": 1.402784940691078,
      "grad_norm": 1.520416259765625,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 9.0498,
      "step": 1360
    },
    {
      "epoch": 1.4234141309953583,
      "grad_norm": 1.1452187299728394,
      "learning_rate": 4.600000000000001e-05,
      "loss": 8.9947,
      "step": 1380
    },
    {
      "epoch": 1.444043321299639,
      "grad_norm": 1.3679643869400024,
      "learning_rate": 4.666666666666667e-05,
      "loss": 9.0603,
      "step": 1400
    },
    {
      "epoch": 1.4646725116039194,
      "grad_norm": 1.378037691116333,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 9.0616,
      "step": 1420
    },
    {
      "epoch": 1.4853017019082002,
      "grad_norm": 2.9862468242645264,
      "learning_rate": 4.8e-05,
      "loss": 8.987,
      "step": 1440
    },
    {
      "epoch": 1.5059308922124806,
      "grad_norm": 2.748274564743042,
      "learning_rate": 4.866666666666667e-05,
      "loss": 9.0787,
      "step": 1460
    },
    {
      "epoch": 1.5265600825167613,
      "grad_norm": 1.8325855731964111,
      "learning_rate": 4.933333333333334e-05,
      "loss": 9.0565,
      "step": 1480
    },
    {
      "epoch": 1.5471892728210417,
      "grad_norm": 1.9332565069198608,
      "learning_rate": 5e-05,
      "loss": 8.9941,
      "step": 1500
    },
    {
      "epoch": 1.5471892728210417,
      "eval_accuracy": 0.008671836957269752,
      "eval_loss": 9.025077819824219,
      "eval_runtime": 70.3232,
      "eval_samples_per_second": 23.62,
      "eval_steps_per_second": 5.916,
      "step": 1500
    },
    {
      "epoch": 1.5678184631253225,
      "grad_norm": 1.3886111974716187,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 9.0161,
      "step": 1520
    },
    {
      "epoch": 1.5884476534296028,
      "grad_norm": 1.9580022096633911,
      "learning_rate": 5.133333333333333e-05,
      "loss": 9.0174,
      "step": 1540
    },
    {
      "epoch": 1.6090768437338836,
      "grad_norm": 1.9498300552368164,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 8.9767,
      "step": 1560
    },
    {
      "epoch": 1.629706034038164,
      "grad_norm": 1.1316841840744019,
      "learning_rate": 5.266666666666666e-05,
      "loss": 8.9683,
      "step": 1580
    },
    {
      "epoch": 1.6503352243424445,
      "grad_norm": 2.051257371902466,
      "learning_rate": 5.333333333333333e-05,
      "loss": 8.9509,
      "step": 1600
    },
    {
      "epoch": 1.670964414646725,
      "grad_norm": 2.5670037269592285,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 8.9932,
      "step": 1620
    },
    {
      "epoch": 1.6915936049510056,
      "grad_norm": 1.6747028827667236,
      "learning_rate": 5.466666666666666e-05,
      "loss": 9.0351,
      "step": 1640
    },
    {
      "epoch": 1.7122227952552862,
      "grad_norm": 1.889462947845459,
      "learning_rate": 5.5333333333333334e-05,
      "loss": 9.045,
      "step": 1660
    },
    {
      "epoch": 1.7328519855595668,
      "grad_norm": 1.376763939857483,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 8.9297,
      "step": 1680
    },
    {
      "epoch": 1.7534811758638473,
      "grad_norm": 2.1159756183624268,
      "learning_rate": 5.666666666666667e-05,
      "loss": 8.9947,
      "step": 1700
    },
    {
      "epoch": 1.774110366168128,
      "grad_norm": 3.8339638710021973,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 9.0017,
      "step": 1720
    },
    {
      "epoch": 1.7947395564724085,
      "grad_norm": 1.9637075662612915,
      "learning_rate": 5.8e-05,
      "loss": 8.9553,
      "step": 1740
    },
    {
      "epoch": 1.815368746776689,
      "grad_norm": 3.519456148147583,
      "learning_rate": 5.866666666666667e-05,
      "loss": 8.9046,
      "step": 1760
    },
    {
      "epoch": 1.8359979370809696,
      "grad_norm": 1.6504446268081665,
      "learning_rate": 5.9333333333333343e-05,
      "loss": 8.9429,
      "step": 1780
    },
    {
      "epoch": 1.8566271273852502,
      "grad_norm": 2.689509391784668,
      "learning_rate": 6e-05,
      "loss": 8.955,
      "step": 1800
    },
    {
      "epoch": 1.8772563176895307,
      "grad_norm": 2.2618675231933594,
      "learning_rate": 6.066666666666667e-05,
      "loss": 8.9119,
      "step": 1820
    },
    {
      "epoch": 1.8978855079938113,
      "grad_norm": 2.4789962768554688,
      "learning_rate": 6.133333333333334e-05,
      "loss": 8.9305,
      "step": 1840
    },
    {
      "epoch": 1.9185146982980918,
      "grad_norm": 1.7290538549423218,
      "learning_rate": 6.2e-05,
      "loss": 8.8935,
      "step": 1860
    },
    {
      "epoch": 1.9391438886023724,
      "grad_norm": 1.941609263420105,
      "learning_rate": 6.266666666666667e-05,
      "loss": 8.9058,
      "step": 1880
    },
    {
      "epoch": 1.959773078906653,
      "grad_norm": 2.21225905418396,
      "learning_rate": 6.333333333333333e-05,
      "loss": 8.963,
      "step": 1900
    },
    {
      "epoch": 1.9804022692109333,
      "grad_norm": 1.724211573600769,
      "learning_rate": 6.400000000000001e-05,
      "loss": 8.8931,
      "step": 1920
    },
    {
      "epoch": 2.001031459515214,
      "grad_norm": 3.001309633255005,
      "learning_rate": 6.466666666666666e-05,
      "loss": 8.9217,
      "step": 1940
    },
    {
      "epoch": 2.0216606498194944,
      "grad_norm": 2.006804943084717,
      "learning_rate": 6.533333333333334e-05,
      "loss": 8.842,
      "step": 1960
    },
    {
      "epoch": 2.0422898401237752,
      "grad_norm": 2.7576944828033447,
      "learning_rate": 6.6e-05,
      "loss": 8.8612,
      "step": 1980
    },
    {
      "epoch": 2.0629190304280556,
      "grad_norm": 1.8240641355514526,
      "learning_rate": 6.666666666666667e-05,
      "loss": 8.8348,
      "step": 2000
    },
    {
      "epoch": 2.0629190304280556,
      "eval_accuracy": 0.01965296972305001,
      "eval_loss": 8.846634864807129,
      "eval_runtime": 70.4962,
      "eval_samples_per_second": 23.562,
      "eval_steps_per_second": 5.901,
      "step": 2000
    },
    {
      "epoch": 2.0835482207323364,
      "grad_norm": 2.5064408779144287,
      "learning_rate": 6.733333333333333e-05,
      "loss": 8.7971,
      "step": 2020
    },
    {
      "epoch": 2.1041774110366167,
      "grad_norm": 1.9038528203964233,
      "learning_rate": 6.800000000000001e-05,
      "loss": 8.7871,
      "step": 2040
    },
    {
      "epoch": 2.1248066013408975,
      "grad_norm": 1.882630705833435,
      "learning_rate": 6.866666666666666e-05,
      "loss": 8.8065,
      "step": 2060
    },
    {
      "epoch": 2.145435791645178,
      "grad_norm": 1.965146541595459,
      "learning_rate": 6.933333333333334e-05,
      "loss": 8.7622,
      "step": 2080
    },
    {
      "epoch": 2.1660649819494586,
      "grad_norm": 2.0879063606262207,
      "learning_rate": 7e-05,
      "loss": 8.7199,
      "step": 2100
    },
    {
      "epoch": 2.186694172253739,
      "grad_norm": 2.2732067108154297,
      "learning_rate": 7.066666666666667e-05,
      "loss": 8.7288,
      "step": 2120
    },
    {
      "epoch": 2.2073233625580198,
      "grad_norm": 2.58003306388855,
      "learning_rate": 7.133333333333334e-05,
      "loss": 8.6644,
      "step": 2140
    },
    {
      "epoch": 2.2279525528623,
      "grad_norm": 2.3906891345977783,
      "learning_rate": 7.2e-05,
      "loss": 8.7175,
      "step": 2160
    },
    {
      "epoch": 2.248581743166581,
      "grad_norm": 2.2022249698638916,
      "learning_rate": 7.266666666666667e-05,
      "loss": 8.6722,
      "step": 2180
    },
    {
      "epoch": 2.2692109334708612,
      "grad_norm": 1.749544620513916,
      "learning_rate": 7.333333333333333e-05,
      "loss": 8.6528,
      "step": 2200
    },
    {
      "epoch": 2.289840123775142,
      "grad_norm": 2.5619168281555176,
      "learning_rate": 7.4e-05,
      "loss": 8.6821,
      "step": 2220
    },
    {
      "epoch": 2.3104693140794224,
      "grad_norm": 2.002804756164551,
      "learning_rate": 7.466666666666667e-05,
      "loss": 8.6243,
      "step": 2240
    },
    {
      "epoch": 2.3310985043837027,
      "grad_norm": 2.6885392665863037,
      "learning_rate": 7.533333333333334e-05,
      "loss": 8.6145,
      "step": 2260
    },
    {
      "epoch": 2.3517276946879835,
      "grad_norm": 2.833559989929199,
      "learning_rate": 7.6e-05,
      "loss": 8.5714,
      "step": 2280
    },
    {
      "epoch": 2.3723568849922643,
      "grad_norm": 3.021613359451294,
      "learning_rate": 7.666666666666667e-05,
      "loss": 8.5708,
      "step": 2300
    },
    {
      "epoch": 2.3929860752965446,
      "grad_norm": 2.1477720737457275,
      "learning_rate": 7.733333333333333e-05,
      "loss": 8.5294,
      "step": 2320
    },
    {
      "epoch": 2.413615265600825,
      "grad_norm": 2.240682363510132,
      "learning_rate": 7.800000000000001e-05,
      "loss": 8.4881,
      "step": 2340
    },
    {
      "epoch": 2.4342444559051057,
      "grad_norm": 2.8260934352874756,
      "learning_rate": 7.866666666666666e-05,
      "loss": 8.4514,
      "step": 2360
    },
    {
      "epoch": 2.4548736462093865,
      "grad_norm": 2.054961681365967,
      "learning_rate": 7.933333333333334e-05,
      "loss": 8.4619,
      "step": 2380
    },
    {
      "epoch": 2.475502836513667,
      "grad_norm": 3.259268283843994,
      "learning_rate": 8e-05,
      "loss": 8.4473,
      "step": 2400
    },
    {
      "epoch": 2.496132026817947,
      "grad_norm": 2.8688488006591797,
      "learning_rate": 8.066666666666667e-05,
      "loss": 8.4357,
      "step": 2420
    },
    {
      "epoch": 2.516761217122228,
      "grad_norm": 2.6790764331817627,
      "learning_rate": 8.133333333333334e-05,
      "loss": 8.371,
      "step": 2440
    },
    {
      "epoch": 2.5373904074265083,
      "grad_norm": 3.353802442550659,
      "learning_rate": 8.2e-05,
      "loss": 8.3203,
      "step": 2460
    },
    {
      "epoch": 2.558019597730789,
      "grad_norm": 2.9461209774017334,
      "learning_rate": 8.266666666666667e-05,
      "loss": 8.3603,
      "step": 2480
    },
    {
      "epoch": 2.5786487880350695,
      "grad_norm": 2.2339835166931152,
      "learning_rate": 8.333333333333334e-05,
      "loss": 8.2952,
      "step": 2500
    },
    {
      "epoch": 2.5786487880350695,
      "eval_accuracy": 0.016861082485676877,
      "eval_loss": 8.27522087097168,
      "eval_runtime": 70.0903,
      "eval_samples_per_second": 23.698,
      "eval_steps_per_second": 5.935,
      "step": 2500
    },
    {
      "epoch": 2.5992779783393503,
      "grad_norm": 2.6134870052337646,
      "learning_rate": 8.4e-05,
      "loss": 8.2493,
      "step": 2520
    },
    {
      "epoch": 2.6199071686436306,
      "grad_norm": 2.6960673332214355,
      "learning_rate": 8.466666666666667e-05,
      "loss": 8.2056,
      "step": 2540
    },
    {
      "epoch": 2.6405363589479114,
      "grad_norm": 4.3131818771362305,
      "learning_rate": 8.533333333333334e-05,
      "loss": 8.2411,
      "step": 2560
    },
    {
      "epoch": 2.6611655492521917,
      "grad_norm": 2.4873154163360596,
      "learning_rate": 8.6e-05,
      "loss": 8.2078,
      "step": 2580
    },
    {
      "epoch": 2.6817947395564725,
      "grad_norm": 4.430240631103516,
      "learning_rate": 8.666666666666667e-05,
      "loss": 8.2061,
      "step": 2600
    },
    {
      "epoch": 2.702423929860753,
      "grad_norm": 2.364114999771118,
      "learning_rate": 8.733333333333333e-05,
      "loss": 8.1895,
      "step": 2620
    },
    {
      "epoch": 2.7230531201650336,
      "grad_norm": 3.0597805976867676,
      "learning_rate": 8.800000000000001e-05,
      "loss": 8.1544,
      "step": 2640
    },
    {
      "epoch": 2.743682310469314,
      "grad_norm": 2.1501305103302,
      "learning_rate": 8.866666666666668e-05,
      "loss": 8.0848,
      "step": 2660
    },
    {
      "epoch": 2.7643115007735948,
      "grad_norm": 2.8317792415618896,
      "learning_rate": 8.933333333333334e-05,
      "loss": 8.1105,
      "step": 2680
    },
    {
      "epoch": 2.784940691077875,
      "grad_norm": 2.3214151859283447,
      "learning_rate": 9e-05,
      "loss": 8.0574,
      "step": 2700
    },
    {
      "epoch": 2.805569881382156,
      "grad_norm": 2.411531448364258,
      "learning_rate": 9.066666666666667e-05,
      "loss": 8.0824,
      "step": 2720
    },
    {
      "epoch": 2.8261990716864362,
      "grad_norm": 3.8821699619293213,
      "learning_rate": 9.133333333333334e-05,
      "loss": 8.0301,
      "step": 2740
    },
    {
      "epoch": 2.8468282619907166,
      "grad_norm": 4.993692874908447,
      "learning_rate": 9.200000000000001e-05,
      "loss": 8.0369,
      "step": 2760
    },
    {
      "epoch": 2.8674574522949974,
      "grad_norm": 2.8075356483459473,
      "learning_rate": 9.266666666666666e-05,
      "loss": 7.9644,
      "step": 2780
    },
    {
      "epoch": 2.888086642599278,
      "grad_norm": 3.883697271347046,
      "learning_rate": 9.333333333333334e-05,
      "loss": 7.9768,
      "step": 2800
    },
    {
      "epoch": 2.9087158329035585,
      "grad_norm": 2.621419906616211,
      "learning_rate": 9.4e-05,
      "loss": 7.8875,
      "step": 2820
    },
    {
      "epoch": 2.929345023207839,
      "grad_norm": 2.7681057453155518,
      "learning_rate": 9.466666666666667e-05,
      "loss": 7.8762,
      "step": 2840
    },
    {
      "epoch": 2.9499742135121196,
      "grad_norm": 4.487601280212402,
      "learning_rate": 9.533333333333334e-05,
      "loss": 7.9023,
      "step": 2860
    },
    {
      "epoch": 2.9706034038164004,
      "grad_norm": 3.2418053150177,
      "learning_rate": 9.6e-05,
      "loss": 7.8909,
      "step": 2880
    },
    {
      "epoch": 2.9912325941206808,
      "grad_norm": 2.732917547225952,
      "learning_rate": 9.666666666666667e-05,
      "loss": 7.8512,
      "step": 2900
    },
    {
      "epoch": 3.011861784424961,
      "grad_norm": 3.2465410232543945,
      "learning_rate": 9.733333333333335e-05,
      "loss": 7.7881,
      "step": 2920
    },
    {
      "epoch": 3.032490974729242,
      "grad_norm": 3.6297812461853027,
      "learning_rate": 9.8e-05,
      "loss": 7.7614,
      "step": 2940
    },
    {
      "epoch": 3.0531201650335222,
      "grad_norm": 3.0364041328430176,
      "learning_rate": 9.866666666666668e-05,
      "loss": 7.7419,
      "step": 2960
    },
    {
      "epoch": 3.073749355337803,
      "grad_norm": 4.427730560302734,
      "learning_rate": 9.933333333333334e-05,
      "loss": 7.7203,
      "step": 2980
    },
    {
      "epoch": 3.0943785456420834,
      "grad_norm": 3.281907320022583,
      "learning_rate": 0.0001,
      "loss": 7.7033,
      "step": 3000
    },
    {
      "epoch": 3.0943785456420834,
      "eval_accuracy": 0.012718984571883038,
      "eval_loss": 7.728157043457031,
      "eval_runtime": 71.5929,
      "eval_samples_per_second": 23.201,
      "eval_steps_per_second": 5.811,
      "step": 3000
    },
    {
      "epoch": 3.115007735946364,
      "grad_norm": 3.3280131816864014,
      "learning_rate": 9.999798580854356e-05,
      "loss": 7.7193,
      "step": 3020
    },
    {
      "epoch": 3.1356369262506445,
      "grad_norm": 3.2455904483795166,
      "learning_rate": 9.999194339645292e-05,
      "loss": 7.6999,
      "step": 3040
    },
    {
      "epoch": 3.1562661165549253,
      "grad_norm": 2.798107624053955,
      "learning_rate": 9.998187325055106e-05,
      "loss": 7.638,
      "step": 3060
    },
    {
      "epoch": 3.1768953068592056,
      "grad_norm": 3.17706298828125,
      "learning_rate": 9.996777618216607e-05,
      "loss": 7.6902,
      "step": 3080
    },
    {
      "epoch": 3.1975244971634864,
      "grad_norm": 2.7034504413604736,
      "learning_rate": 9.994965332706573e-05,
      "loss": 7.6056,
      "step": 3100
    },
    {
      "epoch": 3.2181536874677668,
      "grad_norm": 3.8836073875427246,
      "learning_rate": 9.992750614536605e-05,
      "loss": 7.5897,
      "step": 3120
    },
    {
      "epoch": 3.2387828777720475,
      "grad_norm": 3.2591724395751953,
      "learning_rate": 9.990133642141359e-05,
      "loss": 7.5982,
      "step": 3140
    },
    {
      "epoch": 3.259412068076328,
      "grad_norm": 3.360746383666992,
      "learning_rate": 9.987114626364171e-05,
      "loss": 7.6105,
      "step": 3160
    },
    {
      "epoch": 3.2800412583806087,
      "grad_norm": 4.214905738830566,
      "learning_rate": 9.983693810440073e-05,
      "loss": 7.5005,
      "step": 3180
    },
    {
      "epoch": 3.300670448684889,
      "grad_norm": 2.805060386657715,
      "learning_rate": 9.979871469976196e-05,
      "loss": 7.5165,
      "step": 3200
    },
    {
      "epoch": 3.32129963898917,
      "grad_norm": 4.10759973526001,
      "learning_rate": 9.975647912929556e-05,
      "loss": 7.4922,
      "step": 3220
    },
    {
      "epoch": 3.34192882929345,
      "grad_norm": 2.5711565017700195,
      "learning_rate": 9.971023479582257e-05,
      "loss": 7.4764,
      "step": 3240
    },
    {
      "epoch": 3.362558019597731,
      "grad_norm": 3.3796935081481934,
      "learning_rate": 9.965998542514066e-05,
      "loss": 7.5399,
      "step": 3260
    },
    {
      "epoch": 3.3831872099020113,
      "grad_norm": 3.9200737476348877,
      "learning_rate": 9.96057350657239e-05,
      "loss": 7.4749,
      "step": 3280
    },
    {
      "epoch": 3.403816400206292,
      "grad_norm": 3.323503255844116,
      "learning_rate": 9.954748808839674e-05,
      "loss": 7.3827,
      "step": 3300
    },
    {
      "epoch": 3.4244455905105724,
      "grad_norm": 3.420358180999756,
      "learning_rate": 9.948524918598175e-05,
      "loss": 7.4739,
      "step": 3320
    },
    {
      "epoch": 3.445074780814853,
      "grad_norm": 3.0127077102661133,
      "learning_rate": 9.941902337292155e-05,
      "loss": 7.4907,
      "step": 3340
    },
    {
      "epoch": 3.4657039711191335,
      "grad_norm": 3.1798672676086426,
      "learning_rate": 9.934881598487479e-05,
      "loss": 7.3553,
      "step": 3360
    },
    {
      "epoch": 3.4863331614234143,
      "grad_norm": 3.756273031234741,
      "learning_rate": 9.927463267828633e-05,
      "loss": 7.4511,
      "step": 3380
    },
    {
      "epoch": 3.5069623517276947,
      "grad_norm": 3.7386233806610107,
      "learning_rate": 9.919647942993148e-05,
      "loss": 7.3414,
      "step": 3400
    },
    {
      "epoch": 3.527591542031975,
      "grad_norm": 2.9441561698913574,
      "learning_rate": 9.911436253643445e-05,
      "loss": 7.3627,
      "step": 3420
    },
    {
      "epoch": 3.548220732336256,
      "grad_norm": 3.1516199111938477,
      "learning_rate": 9.902828861376101e-05,
      "loss": 7.4352,
      "step": 3440
    },
    {
      "epoch": 3.5688499226405366,
      "grad_norm": 2.267836332321167,
      "learning_rate": 9.893826459668558e-05,
      "loss": 7.3383,
      "step": 3460
    },
    {
      "epoch": 3.589479112944817,
      "grad_norm": 2.996245861053467,
      "learning_rate": 9.884429773823239e-05,
      "loss": 7.3633,
      "step": 3480
    },
    {
      "epoch": 3.6101083032490973,
      "grad_norm": 3.3134045600891113,
      "learning_rate": 9.874639560909117e-05,
      "loss": 7.351,
      "step": 3500
    },
    {
      "epoch": 3.6101083032490973,
      "eval_accuracy": 0.010790361063756948,
      "eval_loss": 7.3176469802856445,
      "eval_runtime": 69.275,
      "eval_samples_per_second": 23.977,
      "eval_steps_per_second": 6.005,
      "step": 3500
    },
    {
      "epoch": 3.630737493553378,
      "grad_norm": 3.95518159866333,
      "learning_rate": 9.864456609700726e-05,
      "loss": 7.2823,
      "step": 3520
    },
    {
      "epoch": 3.651366683857659,
      "grad_norm": 3.350085496902466,
      "learning_rate": 9.853881740614591e-05,
      "loss": 7.2759,
      "step": 3540
    },
    {
      "epoch": 3.671995874161939,
      "grad_norm": 3.4300436973571777,
      "learning_rate": 9.842915805643155e-05,
      "loss": 7.3396,
      "step": 3560
    },
    {
      "epoch": 3.6926250644662195,
      "grad_norm": 2.631986379623413,
      "learning_rate": 9.831559688286121e-05,
      "loss": 7.1611,
      "step": 3580
    },
    {
      "epoch": 3.7132542547705003,
      "grad_norm": 2.923236131668091,
      "learning_rate": 9.819814303479267e-05,
      "loss": 7.2931,
      "step": 3600
    },
    {
      "epoch": 3.7338834450747806,
      "grad_norm": 2.725044012069702,
      "learning_rate": 9.807680597520746e-05,
      "loss": 7.224,
      "step": 3620
    },
    {
      "epoch": 3.7545126353790614,
      "grad_norm": 4.225573539733887,
      "learning_rate": 9.79515954799483e-05,
      "loss": 7.209,
      "step": 3640
    },
    {
      "epoch": 3.775141825683342,
      "grad_norm": 3.716594696044922,
      "learning_rate": 9.782252163693158e-05,
      "loss": 7.2903,
      "step": 3660
    },
    {
      "epoch": 3.7957710159876226,
      "grad_norm": 3.3207757472991943,
      "learning_rate": 9.76895948453346e-05,
      "loss": 7.2785,
      "step": 3680
    },
    {
      "epoch": 3.816400206291903,
      "grad_norm": 3.6872928142547607,
      "learning_rate": 9.755282581475769e-05,
      "loss": 7.1724,
      "step": 3700
    },
    {
      "epoch": 3.8370293965961837,
      "grad_norm": 3.479546308517456,
      "learning_rate": 9.741222556436132e-05,
      "loss": 7.2212,
      "step": 3720
    },
    {
      "epoch": 3.857658586900464,
      "grad_norm": 4.443289756774902,
      "learning_rate": 9.726780542197844e-05,
      "loss": 7.2629,
      "step": 3740
    },
    {
      "epoch": 3.878287777204745,
      "grad_norm": 3.2328274250030518,
      "learning_rate": 9.711957702320175e-05,
      "loss": 7.1479,
      "step": 3760
    },
    {
      "epoch": 3.898916967509025,
      "grad_norm": 3.3589820861816406,
      "learning_rate": 9.696755231044618e-05,
      "loss": 7.1087,
      "step": 3780
    },
    {
      "epoch": 3.919546157813306,
      "grad_norm": 2.952061176300049,
      "learning_rate": 9.681174353198687e-05,
      "loss": 7.0848,
      "step": 3800
    },
    {
      "epoch": 3.9401753481175863,
      "grad_norm": 4.049703121185303,
      "learning_rate": 9.665216324097222e-05,
      "loss": 7.0985,
      "step": 3820
    },
    {
      "epoch": 3.960804538421867,
      "grad_norm": 4.391391277313232,
      "learning_rate": 9.648882429441257e-05,
      "loss": 7.1165,
      "step": 3840
    },
    {
      "epoch": 3.9814337287261474,
      "grad_norm": 3.2260019779205322,
      "learning_rate": 9.632173985214438e-05,
      "loss": 7.1439,
      "step": 3860
    },
    {
      "epoch": 4.002062919030428,
      "grad_norm": 3.0753121376037598,
      "learning_rate": 9.615092337576988e-05,
      "loss": 7.0661,
      "step": 3880
    },
    {
      "epoch": 4.0226921093347086,
      "grad_norm": 3.229299545288086,
      "learning_rate": 9.597638862757255e-05,
      "loss": 6.9676,
      "step": 3900
    },
    {
      "epoch": 4.043321299638989,
      "grad_norm": 3.677872657775879,
      "learning_rate": 9.579814966940833e-05,
      "loss": 6.9024,
      "step": 3920
    },
    {
      "epoch": 4.06395048994327,
      "grad_norm": 3.5659701824188232,
      "learning_rate": 9.561622086157272e-05,
      "loss": 6.9577,
      "step": 3940
    },
    {
      "epoch": 4.0845796802475505,
      "grad_norm": 4.440659046173096,
      "learning_rate": 9.543061686164373e-05,
      "loss": 6.9623,
      "step": 3960
    },
    {
      "epoch": 4.105208870551831,
      "grad_norm": 3.2369635105133057,
      "learning_rate": 9.524135262330098e-05,
      "loss": 6.9876,
      "step": 3980
    },
    {
      "epoch": 4.125838060856111,
      "grad_norm": 3.440206289291382,
      "learning_rate": 9.504844339512095e-05,
      "loss": 7.0349,
      "step": 4000
    },
    {
      "epoch": 4.125838060856111,
      "eval_accuracy": 0.0071717964509494594,
      "eval_loss": 7.014244556427002,
      "eval_runtime": 70.8373,
      "eval_samples_per_second": 23.448,
      "eval_steps_per_second": 5.873,
      "step": 4000
    },
    {
      "epoch": 4.1464672511603915,
      "grad_norm": 3.7532362937927246,
      "learning_rate": 9.485190471934843e-05,
      "loss": 6.9479,
      "step": 4020
    },
    {
      "epoch": 4.167096441464673,
      "grad_norm": 3.6524407863616943,
      "learning_rate": 9.465175243064428e-05,
      "loss": 7.0003,
      "step": 4040
    },
    {
      "epoch": 4.187725631768953,
      "grad_norm": 3.552170515060425,
      "learning_rate": 9.444800265480967e-05,
      "loss": 6.8894,
      "step": 4060
    },
    {
      "epoch": 4.208354822073233,
      "grad_norm": 4.901673793792725,
      "learning_rate": 9.424067180748692e-05,
      "loss": 6.8786,
      "step": 4080
    },
    {
      "epoch": 4.228984012377514,
      "grad_norm": 5.369170665740967,
      "learning_rate": 9.40297765928369e-05,
      "loss": 6.9192,
      "step": 4100
    },
    {
      "epoch": 4.249613202681795,
      "grad_norm": 3.4459002017974854,
      "learning_rate": 9.381533400219318e-05,
      "loss": 6.8089,
      "step": 4120
    },
    {
      "epoch": 4.270242392986075,
      "grad_norm": 3.412423610687256,
      "learning_rate": 9.359736131269312e-05,
      "loss": 6.9399,
      "step": 4140
    },
    {
      "epoch": 4.290871583290356,
      "grad_norm": 3.8060693740844727,
      "learning_rate": 9.337587608588588e-05,
      "loss": 6.8823,
      "step": 4160
    },
    {
      "epoch": 4.311500773594636,
      "grad_norm": 3.3021578788757324,
      "learning_rate": 9.315089616631752e-05,
      "loss": 6.9563,
      "step": 4180
    },
    {
      "epoch": 4.332129963898917,
      "grad_norm": 4.551305294036865,
      "learning_rate": 9.292243968009331e-05,
      "loss": 6.8486,
      "step": 4200
    },
    {
      "epoch": 4.352759154203198,
      "grad_norm": 4.771883010864258,
      "learning_rate": 9.269052503341736e-05,
      "loss": 6.7993,
      "step": 4220
    },
    {
      "epoch": 4.373388344507478,
      "grad_norm": 3.391939163208008,
      "learning_rate": 9.24551709111097e-05,
      "loss": 6.9097,
      "step": 4240
    },
    {
      "epoch": 4.394017534811758,
      "grad_norm": 4.408979892730713,
      "learning_rate": 9.221639627510076e-05,
      "loss": 6.8889,
      "step": 4260
    },
    {
      "epoch": 4.4146467251160395,
      "grad_norm": 3.6266558170318604,
      "learning_rate": 9.197422036290387e-05,
      "loss": 6.7661,
      "step": 4280
    },
    {
      "epoch": 4.43527591542032,
      "grad_norm": 3.6101021766662598,
      "learning_rate": 9.172866268606513e-05,
      "loss": 6.9387,
      "step": 4300
    },
    {
      "epoch": 4.4559051057246,
      "grad_norm": 3.476651668548584,
      "learning_rate": 9.147974302859157e-05,
      "loss": 6.9207,
      "step": 4320
    },
    {
      "epoch": 4.4765342960288805,
      "grad_norm": 4.338423728942871,
      "learning_rate": 9.122748144535705e-05,
      "loss": 6.8065,
      "step": 4340
    },
    {
      "epoch": 4.497163486333162,
      "grad_norm": 3.2161855697631836,
      "learning_rate": 9.09718982604866e-05,
      "loss": 6.7708,
      "step": 4360
    },
    {
      "epoch": 4.517792676637442,
      "grad_norm": 3.375483512878418,
      "learning_rate": 9.071301406571892e-05,
      "loss": 6.7106,
      "step": 4380
    },
    {
      "epoch": 4.5384218669417224,
      "grad_norm": 4.528444290161133,
      "learning_rate": 9.045084971874738e-05,
      "loss": 6.8716,
      "step": 4400
    },
    {
      "epoch": 4.559051057246003,
      "grad_norm": 3.333712339401245,
      "learning_rate": 9.018542634153944e-05,
      "loss": 6.7858,
      "step": 4420
    },
    {
      "epoch": 4.579680247550284,
      "grad_norm": 3.741593837738037,
      "learning_rate": 8.991676531863508e-05,
      "loss": 6.8731,
      "step": 4440
    },
    {
      "epoch": 4.600309437854564,
      "grad_norm": 4.069019794464111,
      "learning_rate": 8.964488829542377e-05,
      "loss": 6.7824,
      "step": 4460
    },
    {
      "epoch": 4.620938628158845,
      "grad_norm": 3.402919292449951,
      "learning_rate": 8.936981717640061e-05,
      "loss": 6.7901,
      "step": 4480
    },
    {
      "epoch": 4.641567818463125,
      "grad_norm": 4.682876110076904,
      "learning_rate": 8.90915741234015e-05,
      "loss": 6.8791,
      "step": 4500
    },
    {
      "epoch": 4.641567818463125,
      "eval_accuracy": 0.006300692788742553,
      "eval_loss": 6.801791191101074,
      "eval_runtime": 68.8286,
      "eval_samples_per_second": 24.132,
      "eval_steps_per_second": 6.044,
      "step": 4500
    },
    {
      "epoch": 4.662197008767405,
      "grad_norm": 4.047665596008301,
      "learning_rate": 8.881018155381766e-05,
      "loss": 6.702,
      "step": 4520
    },
    {
      "epoch": 4.682826199071687,
      "grad_norm": 4.598515033721924,
      "learning_rate": 8.852566213878947e-05,
      "loss": 6.7749,
      "step": 4540
    },
    {
      "epoch": 4.703455389375967,
      "grad_norm": 3.8549094200134277,
      "learning_rate": 8.823803880137993e-05,
      "loss": 6.812,
      "step": 4560
    },
    {
      "epoch": 4.724084579680247,
      "grad_norm": 4.4460225105285645,
      "learning_rate": 8.794733471472778e-05,
      "loss": 6.6894,
      "step": 4580
    },
    {
      "epoch": 4.7447137699845285,
      "grad_norm": 5.098257541656494,
      "learning_rate": 8.765357330018056e-05,
      "loss": 6.7092,
      "step": 4600
    },
    {
      "epoch": 4.765342960288809,
      "grad_norm": 4.045108318328857,
      "learning_rate": 8.735677822540749e-05,
      "loss": 6.6985,
      "step": 4620
    },
    {
      "epoch": 4.785972150593089,
      "grad_norm": 2.975278615951538,
      "learning_rate": 8.705697340249275e-05,
      "loss": 6.7546,
      "step": 4640
    },
    {
      "epoch": 4.80660134089737,
      "grad_norm": 3.1917836666107178,
      "learning_rate": 8.675418298600884e-05,
      "loss": 6.8909,
      "step": 4660
    },
    {
      "epoch": 4.82723053120165,
      "grad_norm": 3.4950733184814453,
      "learning_rate": 8.644843137107059e-05,
      "loss": 6.6998,
      "step": 4680
    },
    {
      "epoch": 4.847859721505931,
      "grad_norm": 3.7412636280059814,
      "learning_rate": 8.613974319136958e-05,
      "loss": 6.7331,
      "step": 4700
    },
    {
      "epoch": 4.8684889118102115,
      "grad_norm": 4.65611457824707,
      "learning_rate": 8.582814331718961e-05,
      "loss": 6.7104,
      "step": 4720
    },
    {
      "epoch": 4.889118102114492,
      "grad_norm": 3.1899254322052,
      "learning_rate": 8.551365685340285e-05,
      "loss": 6.6969,
      "step": 4740
    },
    {
      "epoch": 4.909747292418773,
      "grad_norm": 3.3505055904388428,
      "learning_rate": 8.519630913744725e-05,
      "loss": 6.7226,
      "step": 4760
    },
    {
      "epoch": 4.930376482723053,
      "grad_norm": 4.138996124267578,
      "learning_rate": 8.487612573728513e-05,
      "loss": 6.6665,
      "step": 4780
    },
    {
      "epoch": 4.951005673027334,
      "grad_norm": 4.310397624969482,
      "learning_rate": 8.455313244934324e-05,
      "loss": 6.7231,
      "step": 4800
    },
    {
      "epoch": 4.971634863331614,
      "grad_norm": 3.4226722717285156,
      "learning_rate": 8.422735529643444e-05,
      "loss": 6.7394,
      "step": 4820
    },
    {
      "epoch": 4.992264053635894,
      "grad_norm": 3.0352301597595215,
      "learning_rate": 8.389882052566105e-05,
      "loss": 6.6978,
      "step": 4840
    },
    {
      "epoch": 5.012893243940176,
      "grad_norm": 3.247889757156372,
      "learning_rate": 8.35675546063002e-05,
      "loss": 6.6068,
      "step": 4860
    },
    {
      "epoch": 5.033522434244456,
      "grad_norm": 3.210649251937866,
      "learning_rate": 8.32335842276713e-05,
      "loss": 6.4772,
      "step": 4880
    },
    {
      "epoch": 5.054151624548736,
      "grad_norm": 2.5283050537109375,
      "learning_rate": 8.289693629698564e-05,
      "loss": 6.5852,
      "step": 4900
    },
    {
      "epoch": 5.074780814853017,
      "grad_norm": 4.6602559089660645,
      "learning_rate": 8.255763793717868e-05,
      "loss": 6.6018,
      "step": 4920
    },
    {
      "epoch": 5.095410005157298,
      "grad_norm": 3.6753947734832764,
      "learning_rate": 8.221571648472472e-05,
      "loss": 6.6005,
      "step": 4940
    },
    {
      "epoch": 5.116039195461578,
      "grad_norm": 3.500962734222412,
      "learning_rate": 8.18711994874345e-05,
      "loss": 6.4991,
      "step": 4960
    },
    {
      "epoch": 5.136668385765859,
      "grad_norm": 3.1389002799987793,
      "learning_rate": 8.152411470223569e-05,
      "loss": 6.4036,
      "step": 4980
    },
    {
      "epoch": 5.157297576070139,
      "grad_norm": 3.6738080978393555,
      "learning_rate": 8.117449009293668e-05,
      "loss": 6.5561,
      "step": 5000
    },
    {
      "epoch": 5.157297576070139,
      "eval_accuracy": 0.006621258936434695,
      "eval_loss": 6.631977558135986,
      "eval_runtime": 70.9085,
      "eval_samples_per_second": 23.425,
      "eval_steps_per_second": 5.867,
      "step": 5000
    },
    {
      "epoch": 5.17792676637442,
      "grad_norm": 3.950812578201294,
      "learning_rate": 8.082235382797349e-05,
      "loss": 6.5361,
      "step": 5020
    },
    {
      "epoch": 5.1985559566787005,
      "grad_norm": 4.3112263679504395,
      "learning_rate": 8.046773427814042e-05,
      "loss": 6.5518,
      "step": 5040
    },
    {
      "epoch": 5.219185146982981,
      "grad_norm": 4.088156223297119,
      "learning_rate": 8.011066001430412e-05,
      "loss": 6.4733,
      "step": 5060
    },
    {
      "epoch": 5.239814337287261,
      "grad_norm": 2.7933459281921387,
      "learning_rate": 7.975115980510187e-05,
      "loss": 6.5534,
      "step": 5080
    },
    {
      "epoch": 5.260443527591542,
      "grad_norm": 4.630918979644775,
      "learning_rate": 7.938926261462366e-05,
      "loss": 6.4862,
      "step": 5100
    },
    {
      "epoch": 5.281072717895823,
      "grad_norm": 3.5418832302093506,
      "learning_rate": 7.902499760007867e-05,
      "loss": 6.4326,
      "step": 5120
    },
    {
      "epoch": 5.301701908200103,
      "grad_norm": 4.689242839813232,
      "learning_rate": 7.865839410944612e-05,
      "loss": 6.5247,
      "step": 5140
    },
    {
      "epoch": 5.3223310985043835,
      "grad_norm": 3.050037384033203,
      "learning_rate": 7.828948167911074e-05,
      "loss": 6.6197,
      "step": 5160
    },
    {
      "epoch": 5.342960288808664,
      "grad_norm": 4.33923864364624,
      "learning_rate": 7.791829003148312e-05,
      "loss": 6.4769,
      "step": 5180
    },
    {
      "epoch": 5.363589479112945,
      "grad_norm": 5.058712482452393,
      "learning_rate": 7.754484907260513e-05,
      "loss": 6.4503,
      "step": 5200
    },
    {
      "epoch": 5.384218669417225,
      "grad_norm": 3.430694341659546,
      "learning_rate": 7.71691888897403e-05,
      "loss": 6.6614,
      "step": 5220
    },
    {
      "epoch": 5.404847859721506,
      "grad_norm": 3.8947300910949707,
      "learning_rate": 7.679133974894983e-05,
      "loss": 6.5305,
      "step": 5240
    },
    {
      "epoch": 5.425477050025787,
      "grad_norm": 3.8147201538085938,
      "learning_rate": 7.641133209265424e-05,
      "loss": 6.541,
      "step": 5260
    },
    {
      "epoch": 5.446106240330067,
      "grad_norm": 3.5757369995117188,
      "learning_rate": 7.602919653718044e-05,
      "loss": 6.4929,
      "step": 5280
    },
    {
      "epoch": 5.466735430634348,
      "grad_norm": 4.242160797119141,
      "learning_rate": 7.564496387029532e-05,
      "loss": 6.5164,
      "step": 5300
    },
    {
      "epoch": 5.487364620938628,
      "grad_norm": 3.4169468879699707,
      "learning_rate": 7.525866504872506e-05,
      "loss": 6.531,
      "step": 5320
    },
    {
      "epoch": 5.507993811242908,
      "grad_norm": 5.036027431488037,
      "learning_rate": 7.48703311956611e-05,
      "loss": 6.5974,
      "step": 5340
    },
    {
      "epoch": 5.5286230015471896,
      "grad_norm": 3.75742769241333,
      "learning_rate": 7.447999359825263e-05,
      "loss": 6.4192,
      "step": 5360
    },
    {
      "epoch": 5.54925219185147,
      "grad_norm": 4.131464958190918,
      "learning_rate": 7.408768370508576e-05,
      "loss": 6.5607,
      "step": 5380
    },
    {
      "epoch": 5.56988138215575,
      "grad_norm": 3.7406651973724365,
      "learning_rate": 7.369343312364993e-05,
      "loss": 6.3927,
      "step": 5400
    },
    {
      "epoch": 5.590510572460031,
      "grad_norm": 3.4119110107421875,
      "learning_rate": 7.329727361779124e-05,
      "loss": 6.5054,
      "step": 5420
    },
    {
      "epoch": 5.611139762764312,
      "grad_norm": 5.251886367797852,
      "learning_rate": 7.289923710515339e-05,
      "loss": 6.4092,
      "step": 5440
    },
    {
      "epoch": 5.631768953068592,
      "grad_norm": 3.371389627456665,
      "learning_rate": 7.249935565460607e-05,
      "loss": 6.4367,
      "step": 5460
    },
    {
      "epoch": 5.6523981433728725,
      "grad_norm": 4.197510242462158,
      "learning_rate": 7.209766148366136e-05,
      "loss": 6.3898,
      "step": 5480
    },
    {
      "epoch": 5.673027333677153,
      "grad_norm": 2.902024984359741,
      "learning_rate": 7.169418695587791e-05,
      "loss": 6.4637,
      "step": 5500
    },
    {
      "epoch": 5.673027333677153,
      "eval_accuracy": 0.007082943877404355,
      "eval_loss": 6.506182670593262,
      "eval_runtime": 72.1855,
      "eval_samples_per_second": 23.01,
      "eval_steps_per_second": 5.763,
      "step": 5500
    },
    {
      "epoch": 5.693656523981434,
      "grad_norm": 4.068917751312256,
      "learning_rate": 7.128896457825364e-05,
      "loss": 6.4947,
      "step": 5520
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 4.843897342681885,
      "learning_rate": 7.088202699860656e-05,
      "loss": 6.5149,
      "step": 5540
    },
    {
      "epoch": 5.734914904589995,
      "grad_norm": 4.256856441497803,
      "learning_rate": 7.047340700294453e-05,
      "loss": 6.3471,
      "step": 5560
    },
    {
      "epoch": 5.755544094894275,
      "grad_norm": 3.7972412109375,
      "learning_rate": 7.006313751282372e-05,
      "loss": 6.3941,
      "step": 5580
    },
    {
      "epoch": 5.776173285198556,
      "grad_norm": 3.9108242988586426,
      "learning_rate": 6.965125158269619e-05,
      "loss": 6.4817,
      "step": 5600
    },
    {
      "epoch": 5.796802475502837,
      "grad_norm": 3.7149338722229004,
      "learning_rate": 6.92377823972468e-05,
      "loss": 6.5011,
      "step": 5620
    },
    {
      "epoch": 5.817431665807117,
      "grad_norm": 4.636054992675781,
      "learning_rate": 6.88227632687196e-05,
      "loss": 6.4146,
      "step": 5640
    },
    {
      "epoch": 5.838060856111397,
      "grad_norm": 4.801067352294922,
      "learning_rate": 6.840622763423391e-05,
      "loss": 6.4068,
      "step": 5660
    },
    {
      "epoch": 5.858690046415678,
      "grad_norm": 4.337803363800049,
      "learning_rate": 6.798820905309036e-05,
      "loss": 6.3883,
      "step": 5680
    },
    {
      "epoch": 5.879319236719959,
      "grad_norm": 6.22988748550415,
      "learning_rate": 6.756874120406714e-05,
      "loss": 6.3911,
      "step": 5700
    },
    {
      "epoch": 5.899948427024239,
      "grad_norm": 3.5658650398254395,
      "learning_rate": 6.714785788270658e-05,
      "loss": 6.3552,
      "step": 5720
    },
    {
      "epoch": 5.92057761732852,
      "grad_norm": 3.6006579399108887,
      "learning_rate": 6.672559299859228e-05,
      "loss": 6.3407,
      "step": 5740
    },
    {
      "epoch": 5.941206807632801,
      "grad_norm": 3.4636240005493164,
      "learning_rate": 6.63019805726171e-05,
      "loss": 6.3883,
      "step": 5760
    },
    {
      "epoch": 5.961835997937081,
      "grad_norm": 3.6294662952423096,
      "learning_rate": 6.587705473424222e-05,
      "loss": 6.3889,
      "step": 5780
    },
    {
      "epoch": 5.9824651882413615,
      "grad_norm": 4.117221832275391,
      "learning_rate": 6.545084971874738e-05,
      "loss": 6.4028,
      "step": 5800
    },
    {
      "epoch": 6.003094378545642,
      "grad_norm": 4.822798728942871,
      "learning_rate": 6.50233998644726e-05,
      "loss": 6.3086,
      "step": 5820
    },
    {
      "epoch": 6.023723568849922,
      "grad_norm": 3.865546703338623,
      "learning_rate": 6.459473961005168e-05,
      "loss": 6.2979,
      "step": 5840
    },
    {
      "epoch": 6.0443527591542034,
      "grad_norm": 3.710070848464966,
      "learning_rate": 6.416490349163748e-05,
      "loss": 6.2095,
      "step": 5860
    },
    {
      "epoch": 6.064981949458484,
      "grad_norm": 3.613234281539917,
      "learning_rate": 6.373392614011952e-05,
      "loss": 6.2142,
      "step": 5880
    },
    {
      "epoch": 6.085611139762764,
      "grad_norm": 3.970132827758789,
      "learning_rate": 6.330184227833376e-05,
      "loss": 6.2473,
      "step": 5900
    },
    {
      "epoch": 6.1062403300670445,
      "grad_norm": 4.003325462341309,
      "learning_rate": 6.286868671826512e-05,
      "loss": 6.255,
      "step": 5920
    },
    {
      "epoch": 6.126869520371326,
      "grad_norm": 3.6618006229400635,
      "learning_rate": 6.243449435824276e-05,
      "loss": 6.278,
      "step": 5940
    },
    {
      "epoch": 6.147498710675606,
      "grad_norm": 4.562103271484375,
      "learning_rate": 6.19993001801283e-05,
      "loss": 6.3043,
      "step": 5960
    },
    {
      "epoch": 6.168127900979886,
      "grad_norm": 4.82405424118042,
      "learning_rate": 6.156313924649761e-05,
      "loss": 6.2889,
      "step": 5980
    },
    {
      "epoch": 6.188757091284167,
      "grad_norm": 5.182987689971924,
      "learning_rate": 6.112604669781572e-05,
      "loss": 6.4259,
      "step": 6000
    },
    {
      "epoch": 6.188757091284167,
      "eval_accuracy": 0.007941852088340364,
      "eval_loss": 6.400195598602295,
      "eval_runtime": 69.8818,
      "eval_samples_per_second": 23.769,
      "eval_steps_per_second": 5.953,
      "step": 6000
    },
    {
      "epoch": 6.209386281588448,
      "grad_norm": 3.9121556282043457,
      "learning_rate": 6.068805774960573e-05,
      "loss": 6.3747,
      "step": 6020
    },
    {
      "epoch": 6.230015471892728,
      "grad_norm": 4.749391078948975,
      "learning_rate": 6.0249207689611533e-05,
      "loss": 6.3008,
      "step": 6040
    },
    {
      "epoch": 6.250644662197009,
      "grad_norm": 4.433669567108154,
      "learning_rate": 5.980953187495476e-05,
      "loss": 6.302,
      "step": 6060
    },
    {
      "epoch": 6.271273852501289,
      "grad_norm": 3.29345703125,
      "learning_rate": 5.9369065729286245e-05,
      "loss": 6.2704,
      "step": 6080
    },
    {
      "epoch": 6.29190304280557,
      "grad_norm": 4.6201066970825195,
      "learning_rate": 5.8927844739931834e-05,
      "loss": 6.3298,
      "step": 6100
    },
    {
      "epoch": 6.312532233109851,
      "grad_norm": 3.694225788116455,
      "learning_rate": 5.8485904455033444e-05,
      "loss": 6.2185,
      "step": 6120
    },
    {
      "epoch": 6.333161423414131,
      "grad_norm": 4.888429164886475,
      "learning_rate": 5.804328048068492e-05,
      "loss": 6.2115,
      "step": 6140
    },
    {
      "epoch": 6.353790613718411,
      "grad_norm": 4.2853498458862305,
      "learning_rate": 5.760000847806337e-05,
      "loss": 6.1916,
      "step": 6160
    },
    {
      "epoch": 6.3744198040226925,
      "grad_norm": 3.4863038063049316,
      "learning_rate": 5.715612416055598e-05,
      "loss": 6.2711,
      "step": 6180
    },
    {
      "epoch": 6.395048994326973,
      "grad_norm": 4.790369987487793,
      "learning_rate": 5.6711663290882776e-05,
      "loss": 6.261,
      "step": 6200
    },
    {
      "epoch": 6.415678184631253,
      "grad_norm": 4.590381145477295,
      "learning_rate": 5.6266661678215216e-05,
      "loss": 6.2818,
      "step": 6220
    },
    {
      "epoch": 6.4363073749355335,
      "grad_norm": 4.4804534912109375,
      "learning_rate": 5.582115517529114e-05,
      "loss": 6.2014,
      "step": 6240
    },
    {
      "epoch": 6.456936565239815,
      "grad_norm": 4.356693267822266,
      "learning_rate": 5.537517967552626e-05,
      "loss": 6.3012,
      "step": 6260
    },
    {
      "epoch": 6.477565755544095,
      "grad_norm": 3.7173871994018555,
      "learning_rate": 5.492877111012218e-05,
      "loss": 6.2605,
      "step": 6280
    },
    {
      "epoch": 6.498194945848375,
      "grad_norm": 4.761284828186035,
      "learning_rate": 5.448196544517168e-05,
      "loss": 6.3025,
      "step": 6300
    },
    {
      "epoch": 6.518824136152656,
      "grad_norm": 3.6328439712524414,
      "learning_rate": 5.403479867876087e-05,
      "loss": 6.3129,
      "step": 6320
    },
    {
      "epoch": 6.539453326456936,
      "grad_norm": 5.1391825675964355,
      "learning_rate": 5.3587306838068964e-05,
      "loss": 6.2734,
      "step": 6340
    },
    {
      "epoch": 6.560082516761217,
      "grad_norm": 4.827568054199219,
      "learning_rate": 5.313952597646568e-05,
      "loss": 6.312,
      "step": 6360
    },
    {
      "epoch": 6.580711707065498,
      "grad_norm": 4.200488567352295,
      "learning_rate": 5.2691492170606415e-05,
      "loss": 6.26,
      "step": 6380
    },
    {
      "epoch": 6.601340897369778,
      "grad_norm": 4.232232570648193,
      "learning_rate": 5.2243241517525754e-05,
      "loss": 6.2388,
      "step": 6400
    },
    {
      "epoch": 6.621970087674059,
      "grad_norm": 4.279674530029297,
      "learning_rate": 5.179481013172912e-05,
      "loss": 6.2018,
      "step": 6420
    },
    {
      "epoch": 6.64259927797834,
      "grad_norm": 3.8336265087127686,
      "learning_rate": 5.1346234142283144e-05,
      "loss": 6.25,
      "step": 6440
    },
    {
      "epoch": 6.66322846828262,
      "grad_norm": 4.2949299812316895,
      "learning_rate": 5.0897549689904865e-05,
      "loss": 6.2043,
      "step": 6460
    },
    {
      "epoch": 6.6838576585869,
      "grad_norm": 4.769170761108398,
      "learning_rate": 5.0448792924049894e-05,
      "loss": 6.2282,
      "step": 6480
    },
    {
      "epoch": 6.704486848891181,
      "grad_norm": 3.775829553604126,
      "learning_rate": 5e-05,
      "loss": 6.1649,
      "step": 6500
    },
    {
      "epoch": 6.704486848891181,
      "eval_accuracy": 0.007263262335481185,
      "eval_loss": 6.317530155181885,
      "eval_runtime": 69.9091,
      "eval_samples_per_second": 23.759,
      "eval_steps_per_second": 5.951,
      "step": 6500
    },
    {
      "epoch": 6.725116039195462,
      "grad_norm": 3.805978775024414,
      "learning_rate": 4.9551207075950104e-05,
      "loss": 6.2135,
      "step": 6520
    },
    {
      "epoch": 6.745745229499742,
      "grad_norm": 3.6988420486450195,
      "learning_rate": 4.9102450310095146e-05,
      "loss": 6.1617,
      "step": 6540
    },
    {
      "epoch": 6.7663744198040225,
      "grad_norm": 3.935398578643799,
      "learning_rate": 4.865376585771687e-05,
      "loss": 6.2957,
      "step": 6560
    },
    {
      "epoch": 6.787003610108303,
      "grad_norm": 5.087912559509277,
      "learning_rate": 4.820518986827089e-05,
      "loss": 6.2672,
      "step": 6580
    },
    {
      "epoch": 6.807632800412584,
      "grad_norm": 3.783383846282959,
      "learning_rate": 4.775675848247427e-05,
      "loss": 6.2173,
      "step": 6600
    },
    {
      "epoch": 6.8282619907168645,
      "grad_norm": 4.549003601074219,
      "learning_rate": 4.7308507829393597e-05,
      "loss": 6.2047,
      "step": 6620
    },
    {
      "epoch": 6.848891181021145,
      "grad_norm": 4.338161945343018,
      "learning_rate": 4.6860474023534335e-05,
      "loss": 6.114,
      "step": 6640
    },
    {
      "epoch": 6.869520371325425,
      "grad_norm": 4.139385223388672,
      "learning_rate": 4.6412693161931034e-05,
      "loss": 6.2262,
      "step": 6660
    },
    {
      "epoch": 6.890149561629706,
      "grad_norm": 8.615739822387695,
      "learning_rate": 4.5965201321239144e-05,
      "loss": 6.204,
      "step": 6680
    },
    {
      "epoch": 6.910778751933987,
      "grad_norm": 4.3466033935546875,
      "learning_rate": 4.551803455482833e-05,
      "loss": 6.1077,
      "step": 6700
    },
    {
      "epoch": 6.931407942238267,
      "grad_norm": 3.8485922813415527,
      "learning_rate": 4.5071228889877825e-05,
      "loss": 6.0576,
      "step": 6720
    },
    {
      "epoch": 6.952037132542547,
      "grad_norm": 3.4042487144470215,
      "learning_rate": 4.4624820324473766e-05,
      "loss": 6.1287,
      "step": 6740
    },
    {
      "epoch": 6.972666322846829,
      "grad_norm": 3.166234016418457,
      "learning_rate": 4.4178844824708864e-05,
      "loss": 6.185,
      "step": 6760
    },
    {
      "epoch": 6.993295513151109,
      "grad_norm": 3.445066213607788,
      "learning_rate": 4.373333832178478e-05,
      "loss": 6.247,
      "step": 6780
    },
    {
      "epoch": 7.013924703455389,
      "grad_norm": 4.899921894073486,
      "learning_rate": 4.328833670911724e-05,
      "loss": 6.1139,
      "step": 6800
    },
    {
      "epoch": 7.03455389375967,
      "grad_norm": 4.388591289520264,
      "learning_rate": 4.284387583944403e-05,
      "loss": 6.1238,
      "step": 6820
    },
    {
      "epoch": 7.055183084063951,
      "grad_norm": 4.327473163604736,
      "learning_rate": 4.239999152193664e-05,
      "loss": 6.1028,
      "step": 6840
    },
    {
      "epoch": 7.075812274368231,
      "grad_norm": 4.296482563018799,
      "learning_rate": 4.195671951931509e-05,
      "loss": 6.0984,
      "step": 6860
    },
    {
      "epoch": 7.096441464672512,
      "grad_norm": 3.514035701751709,
      "learning_rate": 4.151409554496656e-05,
      "loss": 6.1354,
      "step": 6880
    },
    {
      "epoch": 7.117070654976792,
      "grad_norm": 3.482862710952759,
      "learning_rate": 4.107215526006817e-05,
      "loss": 6.0247,
      "step": 6900
    },
    {
      "epoch": 7.137699845281073,
      "grad_norm": 4.303101062774658,
      "learning_rate": 4.063093427071376e-05,
      "loss": 6.1357,
      "step": 6920
    },
    {
      "epoch": 7.1583290355853535,
      "grad_norm": 4.0841288566589355,
      "learning_rate": 4.019046812504526e-05,
      "loss": 6.1166,
      "step": 6940
    },
    {
      "epoch": 7.178958225889634,
      "grad_norm": 4.675373554229736,
      "learning_rate": 3.9750792310388485e-05,
      "loss": 6.0825,
      "step": 6960
    },
    {
      "epoch": 7.199587416193914,
      "grad_norm": 3.696873188018799,
      "learning_rate": 3.9311942250394276e-05,
      "loss": 6.1957,
      "step": 6980
    },
    {
      "epoch": 7.2202166064981945,
      "grad_norm": 4.065609455108643,
      "learning_rate": 3.887395330218429e-05,
      "loss": 6.2559,
      "step": 7000
    },
    {
      "epoch": 7.2202166064981945,
      "eval_accuracy": 0.007488878183992773,
      "eval_loss": 6.252682209014893,
      "eval_runtime": 70.2215,
      "eval_samples_per_second": 23.654,
      "eval_steps_per_second": 5.924,
      "step": 7000
    },
    {
      "epoch": 7.240845796802476,
      "grad_norm": 4.228909015655518,
      "learning_rate": 3.84368607535024e-05,
      "loss": 6.022,
      "step": 7020
    },
    {
      "epoch": 7.261474987106756,
      "grad_norm": 3.231079578399658,
      "learning_rate": 3.8000699819871705e-05,
      "loss": 6.0547,
      "step": 7040
    },
    {
      "epoch": 7.282104177411036,
      "grad_norm": 4.184528350830078,
      "learning_rate": 3.756550564175727e-05,
      "loss": 6.0776,
      "step": 7060
    },
    {
      "epoch": 7.302733367715317,
      "grad_norm": 3.870051860809326,
      "learning_rate": 3.713131328173489e-05,
      "loss": 6.1519,
      "step": 7080
    },
    {
      "epoch": 7.323362558019598,
      "grad_norm": 4.146505355834961,
      "learning_rate": 3.6698157721666246e-05,
      "loss": 6.1267,
      "step": 7100
    },
    {
      "epoch": 7.343991748323878,
      "grad_norm": 3.3180341720581055,
      "learning_rate": 3.62660738598805e-05,
      "loss": 6.0652,
      "step": 7120
    },
    {
      "epoch": 7.364620938628159,
      "grad_norm": 5.0858330726623535,
      "learning_rate": 3.5835096508362545e-05,
      "loss": 6.0484,
      "step": 7140
    },
    {
      "epoch": 7.385250128932439,
      "grad_norm": 5.280121326446533,
      "learning_rate": 3.5405260389948336e-05,
      "loss": 6.113,
      "step": 7160
    },
    {
      "epoch": 7.40587931923672,
      "grad_norm": 4.0364532470703125,
      "learning_rate": 3.4976600135527406e-05,
      "loss": 6.1101,
      "step": 7180
    },
    {
      "epoch": 7.426508509541001,
      "grad_norm": 3.8942465782165527,
      "learning_rate": 3.4549150281252636e-05,
      "loss": 6.0974,
      "step": 7200
    },
    {
      "epoch": 7.447137699845281,
      "grad_norm": 4.165672302246094,
      "learning_rate": 3.412294526575779e-05,
      "loss": 6.0015,
      "step": 7220
    },
    {
      "epoch": 7.467766890149561,
      "grad_norm": 4.183584690093994,
      "learning_rate": 3.369801942738291e-05,
      "loss": 6.0028,
      "step": 7240
    },
    {
      "epoch": 7.4883960804538425,
      "grad_norm": 4.033224582672119,
      "learning_rate": 3.3274407001407735e-05,
      "loss": 6.0261,
      "step": 7260
    },
    {
      "epoch": 7.509025270758123,
      "grad_norm": 3.887097120285034,
      "learning_rate": 3.2852142117293435e-05,
      "loss": 6.069,
      "step": 7280
    },
    {
      "epoch": 7.529654461062403,
      "grad_norm": 4.422068119049072,
      "learning_rate": 3.243125879593286e-05,
      "loss": 6.0801,
      "step": 7300
    },
    {
      "epoch": 7.550283651366684,
      "grad_norm": 3.9036898612976074,
      "learning_rate": 3.201179094690967e-05,
      "loss": 6.1195,
      "step": 7320
    },
    {
      "epoch": 7.570912841670965,
      "grad_norm": 3.809462547302246,
      "learning_rate": 3.1593772365766105e-05,
      "loss": 6.0354,
      "step": 7340
    },
    {
      "epoch": 7.591542031975245,
      "grad_norm": 3.8567068576812744,
      "learning_rate": 3.11772367312804e-05,
      "loss": 6.1519,
      "step": 7360
    },
    {
      "epoch": 7.6121712222795255,
      "grad_norm": 4.470500946044922,
      "learning_rate": 3.076221760275321e-05,
      "loss": 6.1003,
      "step": 7380
    },
    {
      "epoch": 7.632800412583806,
      "grad_norm": 3.7564849853515625,
      "learning_rate": 3.0348748417303823e-05,
      "loss": 6.0357,
      "step": 7400
    },
    {
      "epoch": 7.653429602888087,
      "grad_norm": 3.6082119941711426,
      "learning_rate": 2.9936862487176293e-05,
      "loss": 6.1677,
      "step": 7420
    },
    {
      "epoch": 7.674058793192367,
      "grad_norm": 3.863476514816284,
      "learning_rate": 2.9526592997055487e-05,
      "loss": 6.0553,
      "step": 7440
    },
    {
      "epoch": 7.694687983496648,
      "grad_norm": 6.156431198120117,
      "learning_rate": 2.911797300139345e-05,
      "loss": 6.1609,
      "step": 7460
    },
    {
      "epoch": 7.715317173800928,
      "grad_norm": 4.407658576965332,
      "learning_rate": 2.8711035421746367e-05,
      "loss": 6.1449,
      "step": 7480
    },
    {
      "epoch": 7.735946364105208,
      "grad_norm": 4.327150344848633,
      "learning_rate": 2.8305813044122097e-05,
      "loss": 6.0896,
      "step": 7500
    },
    {
      "epoch": 7.735946364105208,
      "eval_accuracy": 0.00824151174813954,
      "eval_loss": 6.2038726806640625,
      "eval_runtime": 71.5694,
      "eval_samples_per_second": 23.208,
      "eval_steps_per_second": 5.813,
      "step": 7500
    },
    {
      "epoch": 7.75657555440949,
      "grad_norm": 3.8284566402435303,
      "learning_rate": 2.7902338516338677e-05,
      "loss": 6.039,
      "step": 7520
    },
    {
      "epoch": 7.77720474471377,
      "grad_norm": 4.517612457275391,
      "learning_rate": 2.7500644345393943e-05,
      "loss": 5.9397,
      "step": 7540
    },
    {
      "epoch": 7.79783393501805,
      "grad_norm": 5.26022481918335,
      "learning_rate": 2.710076289484663e-05,
      "loss": 6.134,
      "step": 7560
    },
    {
      "epoch": 7.818463125322332,
      "grad_norm": 4.77540397644043,
      "learning_rate": 2.6702726382208776e-05,
      "loss": 6.0068,
      "step": 7580
    },
    {
      "epoch": 7.839092315626612,
      "grad_norm": 5.605193614959717,
      "learning_rate": 2.630656687635007e-05,
      "loss": 6.1377,
      "step": 7600
    },
    {
      "epoch": 7.859721505930892,
      "grad_norm": 4.747427940368652,
      "learning_rate": 2.591231629491423e-05,
      "loss": 6.051,
      "step": 7620
    },
    {
      "epoch": 7.880350696235173,
      "grad_norm": 5.068393230438232,
      "learning_rate": 2.5520006401747398e-05,
      "loss": 6.0728,
      "step": 7640
    },
    {
      "epoch": 7.900979886539453,
      "grad_norm": 5.47941780090332,
      "learning_rate": 2.5129668804338906e-05,
      "loss": 6.08,
      "step": 7660
    },
    {
      "epoch": 7.921609076843734,
      "grad_norm": 4.088740348815918,
      "learning_rate": 2.4741334951274947e-05,
      "loss": 6.2044,
      "step": 7680
    },
    {
      "epoch": 7.9422382671480145,
      "grad_norm": 4.319092273712158,
      "learning_rate": 2.43550361297047e-05,
      "loss": 5.9861,
      "step": 7700
    },
    {
      "epoch": 7.962867457452295,
      "grad_norm": 3.4501090049743652,
      "learning_rate": 2.3970803462819584e-05,
      "loss": 5.9939,
      "step": 7720
    },
    {
      "epoch": 7.983496647756575,
      "grad_norm": 4.488390922546387,
      "learning_rate": 2.3588667907345786e-05,
      "loss": 6.0898,
      "step": 7740
    },
    {
      "epoch": 8.004125838060856,
      "grad_norm": 3.9509437084198,
      "learning_rate": 2.3208660251050158e-05,
      "loss": 6.0753,
      "step": 7760
    },
    {
      "epoch": 8.024755028365137,
      "grad_norm": 4.230082988739014,
      "learning_rate": 2.283081111025973e-05,
      "loss": 5.9252,
      "step": 7780
    },
    {
      "epoch": 8.045384218669417,
      "grad_norm": 3.3651366233825684,
      "learning_rate": 2.245515092739488e-05,
      "loss": 6.0653,
      "step": 7800
    },
    {
      "epoch": 8.066013408973697,
      "grad_norm": 3.944118022918701,
      "learning_rate": 2.2081709968516866e-05,
      "loss": 5.9718,
      "step": 7820
    },
    {
      "epoch": 8.086642599277978,
      "grad_norm": 5.089840888977051,
      "learning_rate": 2.1710518320889278e-05,
      "loss": 5.9907,
      "step": 7840
    },
    {
      "epoch": 8.107271789582258,
      "grad_norm": 4.2765421867370605,
      "learning_rate": 2.1341605890553896e-05,
      "loss": 5.9492,
      "step": 7860
    },
    {
      "epoch": 8.12790097988654,
      "grad_norm": 4.23687219619751,
      "learning_rate": 2.0975002399921324e-05,
      "loss": 5.9807,
      "step": 7880
    },
    {
      "epoch": 8.14853017019082,
      "grad_norm": 4.176878929138184,
      "learning_rate": 2.061073738537635e-05,
      "loss": 6.0451,
      "step": 7900
    },
    {
      "epoch": 8.169159360495101,
      "grad_norm": 4.705695152282715,
      "learning_rate": 2.0248840194898156e-05,
      "loss": 6.1268,
      "step": 7920
    },
    {
      "epoch": 8.189788550799381,
      "grad_norm": 4.583559513092041,
      "learning_rate": 1.9889339985695893e-05,
      "loss": 6.0335,
      "step": 7940
    },
    {
      "epoch": 8.210417741103662,
      "grad_norm": 3.867556095123291,
      "learning_rate": 1.95322657218596e-05,
      "loss": 6.0078,
      "step": 7960
    },
    {
      "epoch": 8.231046931407942,
      "grad_norm": 4.076052188873291,
      "learning_rate": 1.9177646172026513e-05,
      "loss": 5.9792,
      "step": 7980
    },
    {
      "epoch": 8.251676121712222,
      "grad_norm": 3.3427178859710693,
      "learning_rate": 1.8825509907063327e-05,
      "loss": 5.9329,
      "step": 8000
    },
    {
      "epoch": 8.251676121712222,
      "eval_accuracy": 0.008332977632671266,
      "eval_loss": 6.170519828796387,
      "eval_runtime": 69.5639,
      "eval_samples_per_second": 23.877,
      "eval_steps_per_second": 5.98,
      "step": 8000
    },
    {
      "epoch": 8.272305312016503,
      "grad_norm": 4.385346412658691,
      "learning_rate": 1.8475885297764305e-05,
      "loss": 6.0793,
      "step": 8020
    },
    {
      "epoch": 8.292934502320783,
      "grad_norm": 4.372013092041016,
      "learning_rate": 1.8128800512565513e-05,
      "loss": 5.9696,
      "step": 8040
    },
    {
      "epoch": 8.313563692625065,
      "grad_norm": 4.829660415649414,
      "learning_rate": 1.778428351527529e-05,
      "loss": 5.7951,
      "step": 8060
    },
    {
      "epoch": 8.334192882929345,
      "grad_norm": 5.846461772918701,
      "learning_rate": 1.744236206282132e-05,
      "loss": 5.958,
      "step": 8080
    },
    {
      "epoch": 8.354822073233626,
      "grad_norm": 3.884620428085327,
      "learning_rate": 1.7103063703014376e-05,
      "loss": 5.9427,
      "step": 8100
    },
    {
      "epoch": 8.375451263537906,
      "grad_norm": 3.6079187393188477,
      "learning_rate": 1.676641577232873e-05,
      "loss": 5.9908,
      "step": 8120
    },
    {
      "epoch": 8.396080453842186,
      "grad_norm": 5.403895378112793,
      "learning_rate": 1.64324453936998e-05,
      "loss": 5.7941,
      "step": 8140
    },
    {
      "epoch": 8.416709644146467,
      "grad_norm": 3.835279703140259,
      "learning_rate": 1.610117947433897e-05,
      "loss": 6.0392,
      "step": 8160
    },
    {
      "epoch": 8.437338834450747,
      "grad_norm": 3.731034278869629,
      "learning_rate": 1.5772644703565565e-05,
      "loss": 6.0364,
      "step": 8180
    },
    {
      "epoch": 8.457968024755028,
      "grad_norm": 3.558791160583496,
      "learning_rate": 1.5446867550656775e-05,
      "loss": 5.9389,
      "step": 8200
    },
    {
      "epoch": 8.47859721505931,
      "grad_norm": 5.013006687164307,
      "learning_rate": 1.5123874262714893e-05,
      "loss": 5.9643,
      "step": 8220
    },
    {
      "epoch": 8.49922640536359,
      "grad_norm": 3.793416976928711,
      "learning_rate": 1.4803690862552755e-05,
      "loss": 5.9522,
      "step": 8240
    },
    {
      "epoch": 8.51985559566787,
      "grad_norm": 4.137521743774414,
      "learning_rate": 1.4486343146597154e-05,
      "loss": 5.8482,
      "step": 8260
    },
    {
      "epoch": 8.54048478597215,
      "grad_norm": 3.4319934844970703,
      "learning_rate": 1.4171856682810386e-05,
      "loss": 6.0547,
      "step": 8280
    },
    {
      "epoch": 8.561113976276431,
      "grad_norm": 3.8666775226593018,
      "learning_rate": 1.3860256808630428e-05,
      "loss": 6.0187,
      "step": 8300
    },
    {
      "epoch": 8.581743166580711,
      "grad_norm": 4.526212692260742,
      "learning_rate": 1.3551568628929434e-05,
      "loss": 5.9665,
      "step": 8320
    },
    {
      "epoch": 8.602372356884992,
      "grad_norm": 2.972682476043701,
      "learning_rate": 1.3245817013991162e-05,
      "loss": 6.0481,
      "step": 8340
    },
    {
      "epoch": 8.623001547189272,
      "grad_norm": 3.0935733318328857,
      "learning_rate": 1.2943026597507269e-05,
      "loss": 6.0646,
      "step": 8360
    },
    {
      "epoch": 8.643630737493554,
      "grad_norm": 4.490087509155273,
      "learning_rate": 1.2643221774592518e-05,
      "loss": 5.9134,
      "step": 8380
    },
    {
      "epoch": 8.664259927797834,
      "grad_norm": 4.595990180969238,
      "learning_rate": 1.2346426699819458e-05,
      "loss": 5.9567,
      "step": 8400
    },
    {
      "epoch": 8.684889118102115,
      "grad_norm": 4.964392185211182,
      "learning_rate": 1.205266528527223e-05,
      "loss": 5.9632,
      "step": 8420
    },
    {
      "epoch": 8.705518308406395,
      "grad_norm": 4.464058876037598,
      "learning_rate": 1.176196119862008e-05,
      "loss": 5.8933,
      "step": 8440
    },
    {
      "epoch": 8.726147498710676,
      "grad_norm": 3.3803257942199707,
      "learning_rate": 1.1474337861210543e-05,
      "loss": 6.0361,
      "step": 8460
    },
    {
      "epoch": 8.746776689014956,
      "grad_norm": 5.14445686340332,
      "learning_rate": 1.118981844618236e-05,
      "loss": 6.0681,
      "step": 8480
    },
    {
      "epoch": 8.767405879319236,
      "grad_norm": 4.7365264892578125,
      "learning_rate": 1.0908425876598516e-05,
      "loss": 5.8891,
      "step": 8500
    },
    {
      "epoch": 8.767405879319236,
      "eval_accuracy": 0.008875675214226168,
      "eval_loss": 6.145549774169922,
      "eval_runtime": 70.2548,
      "eval_samples_per_second": 23.643,
      "eval_steps_per_second": 5.921,
      "step": 8500
    },
    {
      "epoch": 8.788035069623517,
      "grad_norm": 3.553302526473999,
      "learning_rate": 1.0630182823599399e-05,
      "loss": 5.9267,
      "step": 8520
    },
    {
      "epoch": 8.808664259927799,
      "grad_norm": 3.5410900115966797,
      "learning_rate": 1.0355111704576238e-05,
      "loss": 6.0994,
      "step": 8540
    },
    {
      "epoch": 8.829293450232079,
      "grad_norm": 3.4016456604003906,
      "learning_rate": 1.0083234681364934e-05,
      "loss": 6.0738,
      "step": 8560
    },
    {
      "epoch": 8.84992264053636,
      "grad_norm": 4.838191986083984,
      "learning_rate": 9.814573658460562e-06,
      "loss": 5.953,
      "step": 8580
    },
    {
      "epoch": 8.87055183084064,
      "grad_norm": 4.649527549743652,
      "learning_rate": 9.549150281252633e-06,
      "loss": 6.0763,
      "step": 8600
    },
    {
      "epoch": 8.89118102114492,
      "grad_norm": 5.966151237487793,
      "learning_rate": 9.286985934281078e-06,
      "loss": 6.0686,
      "step": 8620
    },
    {
      "epoch": 8.9118102114492,
      "grad_norm": 4.042913913726807,
      "learning_rate": 9.028101739513406e-06,
      "loss": 5.991,
      "step": 8640
    },
    {
      "epoch": 8.93243940175348,
      "grad_norm": 4.707850456237793,
      "learning_rate": 8.772518554642973e-06,
      "loss": 5.9468,
      "step": 8660
    },
    {
      "epoch": 8.953068592057761,
      "grad_norm": 3.1816906929016113,
      "learning_rate": 8.520256971408453e-06,
      "loss": 5.994,
      "step": 8680
    },
    {
      "epoch": 8.973697782362041,
      "grad_norm": 3.622087001800537,
      "learning_rate": 8.271337313934874e-06,
      "loss": 5.9869,
      "step": 8700
    },
    {
      "epoch": 8.994326972666324,
      "grad_norm": 5.224801540374756,
      "learning_rate": 8.025779637096137e-06,
      "loss": 6.0593,
      "step": 8720
    },
    {
      "epoch": 9.014956162970604,
      "grad_norm": 4.9281744956970215,
      "learning_rate": 7.783603724899257e-06,
      "loss": 5.8688,
      "step": 8740
    },
    {
      "epoch": 9.035585353274884,
      "grad_norm": 3.667402744293213,
      "learning_rate": 7.544829088890326e-06,
      "loss": 5.9099,
      "step": 8760
    },
    {
      "epoch": 9.056214543579165,
      "grad_norm": 5.003742694854736,
      "learning_rate": 7.309474966582636e-06,
      "loss": 5.8456,
      "step": 8780
    },
    {
      "epoch": 9.076843733883445,
      "grad_norm": 3.5553221702575684,
      "learning_rate": 7.077560319906695e-06,
      "loss": 5.9532,
      "step": 8800
    },
    {
      "epoch": 9.097472924187725,
      "grad_norm": 4.282025337219238,
      "learning_rate": 6.849103833682491e-06,
      "loss": 5.9707,
      "step": 8820
    },
    {
      "epoch": 9.118102114492006,
      "grad_norm": 3.9112584590911865,
      "learning_rate": 6.624123914114122e-06,
      "loss": 6.0122,
      "step": 8840
    },
    {
      "epoch": 9.138731304796286,
      "grad_norm": 4.135088920593262,
      "learning_rate": 6.402638687306872e-06,
      "loss": 6.0281,
      "step": 8860
    },
    {
      "epoch": 9.159360495100568,
      "grad_norm": 3.830172538757324,
      "learning_rate": 6.1846659978068265e-06,
      "loss": 5.8767,
      "step": 8880
    },
    {
      "epoch": 9.179989685404848,
      "grad_norm": 4.052587509155273,
      "learning_rate": 5.9702234071631e-06,
      "loss": 5.9109,
      "step": 8900
    },
    {
      "epoch": 9.200618875709129,
      "grad_norm": 3.8920340538024902,
      "learning_rate": 5.759328192513075e-06,
      "loss": 5.9166,
      "step": 8920
    },
    {
      "epoch": 9.221248066013409,
      "grad_norm": 4.526488780975342,
      "learning_rate": 5.5519973451903405e-06,
      "loss": 5.9512,
      "step": 8940
    },
    {
      "epoch": 9.24187725631769,
      "grad_norm": 4.483331680297852,
      "learning_rate": 5.348247569355735e-06,
      "loss": 5.874,
      "step": 8960
    },
    {
      "epoch": 9.26250644662197,
      "grad_norm": 3.915595531463623,
      "learning_rate": 5.148095280651566e-06,
      "loss": 5.9635,
      "step": 8980
    },
    {
      "epoch": 9.28313563692625,
      "grad_norm": 3.3375232219696045,
      "learning_rate": 4.951556604879048e-06,
      "loss": 5.9071,
      "step": 9000
    },
    {
      "epoch": 9.28313563692625,
      "eval_accuracy": 0.008696227859811546,
      "eval_loss": 6.1337103843688965,
      "eval_runtime": 71.4992,
      "eval_samples_per_second": 23.231,
      "eval_steps_per_second": 5.818,
      "step": 9000
    },
    {
      "epoch": 9.30376482723053,
      "grad_norm": 4.93072509765625,
      "learning_rate": 4.758647376699032e-06,
      "loss": 5.9714,
      "step": 9020
    },
    {
      "epoch": 9.324394017534813,
      "grad_norm": 4.732515335083008,
      "learning_rate": 4.569383138356276e-06,
      "loss": 5.9984,
      "step": 9040
    },
    {
      "epoch": 9.345023207839093,
      "grad_norm": 6.1137518882751465,
      "learning_rate": 4.38377913842728e-06,
      "loss": 5.9039,
      "step": 9060
    },
    {
      "epoch": 9.365652398143373,
      "grad_norm": 3.6973769664764404,
      "learning_rate": 4.2018503305916775e-06,
      "loss": 5.9773,
      "step": 9080
    },
    {
      "epoch": 9.386281588447654,
      "grad_norm": 5.264009952545166,
      "learning_rate": 4.023611372427471e-06,
      "loss": 6.018,
      "step": 9100
    },
    {
      "epoch": 9.406910778751934,
      "grad_norm": 4.884394645690918,
      "learning_rate": 3.8490766242301355e-06,
      "loss": 6.0408,
      "step": 9120
    },
    {
      "epoch": 9.427539969056214,
      "grad_norm": 4.025516033172607,
      "learning_rate": 3.6782601478556278e-06,
      "loss": 5.9194,
      "step": 9140
    },
    {
      "epoch": 9.448169159360495,
      "grad_norm": 3.790900230407715,
      "learning_rate": 3.511175705587433e-06,
      "loss": 5.8452,
      "step": 9160
    },
    {
      "epoch": 9.468798349664775,
      "grad_norm": 4.404625415802002,
      "learning_rate": 3.3478367590277894e-06,
      "loss": 5.9702,
      "step": 9180
    },
    {
      "epoch": 9.489427539969057,
      "grad_norm": 3.6700901985168457,
      "learning_rate": 3.18825646801314e-06,
      "loss": 6.023,
      "step": 9200
    },
    {
      "epoch": 9.510056730273337,
      "grad_norm": 4.243236541748047,
      "learning_rate": 3.03244768955383e-06,
      "loss": 5.9857,
      "step": 9220
    },
    {
      "epoch": 9.530685920577618,
      "grad_norm": 4.223404407501221,
      "learning_rate": 2.8804229767982637e-06,
      "loss": 5.8965,
      "step": 9240
    },
    {
      "epoch": 9.551315110881898,
      "grad_norm": 3.615831136703491,
      "learning_rate": 2.7321945780215573e-06,
      "loss": 5.9751,
      "step": 9260
    },
    {
      "epoch": 9.571944301186178,
      "grad_norm": 4.235001087188721,
      "learning_rate": 2.587774435638679e-06,
      "loss": 5.9435,
      "step": 9280
    },
    {
      "epoch": 9.592573491490459,
      "grad_norm": 4.059289932250977,
      "learning_rate": 2.4471741852423237e-06,
      "loss": 6.0214,
      "step": 9300
    },
    {
      "epoch": 9.61320268179474,
      "grad_norm": 4.240381240844727,
      "learning_rate": 2.3104051546654013e-06,
      "loss": 6.0552,
      "step": 9320
    },
    {
      "epoch": 9.63383187209902,
      "grad_norm": 3.927851438522339,
      "learning_rate": 2.177478363068425e-06,
      "loss": 5.9316,
      "step": 9340
    },
    {
      "epoch": 9.6544610624033,
      "grad_norm": 3.486274480819702,
      "learning_rate": 2.048404520051722e-06,
      "loss": 5.9403,
      "step": 9360
    },
    {
      "epoch": 9.675090252707582,
      "grad_norm": 3.7752015590667725,
      "learning_rate": 1.9231940247925573e-06,
      "loss": 5.9847,
      "step": 9380
    },
    {
      "epoch": 9.695719443011862,
      "grad_norm": 4.156726837158203,
      "learning_rate": 1.8018569652073381e-06,
      "loss": 6.0178,
      "step": 9400
    },
    {
      "epoch": 9.716348633316143,
      "grad_norm": 4.426032066345215,
      "learning_rate": 1.6844031171388053e-06,
      "loss": 5.9035,
      "step": 9420
    },
    {
      "epoch": 9.736977823620423,
      "grad_norm": 6.410342693328857,
      "learning_rate": 1.5708419435684462e-06,
      "loss": 5.9045,
      "step": 9440
    },
    {
      "epoch": 9.757607013924703,
      "grad_norm": 4.093693733215332,
      "learning_rate": 1.4611825938540935e-06,
      "loss": 5.9748,
      "step": 9460
    },
    {
      "epoch": 9.778236204228984,
      "grad_norm": 3.953234910964966,
      "learning_rate": 1.3554339029927532e-06,
      "loss": 5.9385,
      "step": 9480
    },
    {
      "epoch": 9.798865394533264,
      "grad_norm": 5.51059103012085,
      "learning_rate": 1.2536043909088191e-06,
      "loss": 6.0181,
      "step": 9500
    },
    {
      "epoch": 9.798865394533264,
      "eval_accuracy": 0.008524620438356785,
      "eval_loss": 6.1284050941467285,
      "eval_runtime": 71.358,
      "eval_samples_per_second": 23.277,
      "eval_steps_per_second": 5.83,
      "step": 9500
    },
    {
      "epoch": 9.819494584837544,
      "grad_norm": 4.446058750152588,
      "learning_rate": 1.1557022617676215e-06,
      "loss": 6.0441,
      "step": 9520
    },
    {
      "epoch": 9.840123775141826,
      "grad_norm": 3.7089004516601562,
      "learning_rate": 1.061735403314429e-06,
      "loss": 5.8838,
      "step": 9540
    },
    {
      "epoch": 9.860752965446107,
      "grad_norm": 3.8085734844207764,
      "learning_rate": 9.717113862389992e-07,
      "loss": 5.8754,
      "step": 9560
    },
    {
      "epoch": 9.881382155750387,
      "grad_norm": 3.6920106410980225,
      "learning_rate": 8.856374635655695e-07,
      "loss": 5.8431,
      "step": 9580
    },
    {
      "epoch": 9.902011346054667,
      "grad_norm": 4.579423904418945,
      "learning_rate": 8.035205700685167e-07,
      "loss": 5.9581,
      "step": 9600
    },
    {
      "epoch": 9.922640536358948,
      "grad_norm": 3.3284621238708496,
      "learning_rate": 7.253673217136658e-07,
      "loss": 5.9598,
      "step": 9620
    },
    {
      "epoch": 9.943269726663228,
      "grad_norm": 3.6112396717071533,
      "learning_rate": 6.511840151252169e-07,
      "loss": 6.0027,
      "step": 9640
    },
    {
      "epoch": 9.963898916967509,
      "grad_norm": 4.176403045654297,
      "learning_rate": 5.809766270784666e-07,
      "loss": 5.9829,
      "step": 9660
    },
    {
      "epoch": 9.984528107271789,
      "grad_norm": 4.138515472412109,
      "learning_rate": 5.147508140182556e-07,
      "loss": 5.8973,
      "step": 9680
    },
    {
      "epoch": 10.005157297576071,
      "grad_norm": 3.961822032928467,
      "learning_rate": 4.52511911603265e-07,
      "loss": 5.9843,
      "step": 9700
    },
    {
      "epoch": 10.025786487880351,
      "grad_norm": 4.196067810058594,
      "learning_rate": 3.9426493427611177e-07,
      "loss": 5.9416,
      "step": 9720
    },
    {
      "epoch": 10.046415678184632,
      "grad_norm": 4.115664005279541,
      "learning_rate": 3.4001457485935416e-07,
      "loss": 5.8999,
      "step": 9740
    },
    {
      "epoch": 10.067044868488912,
      "grad_norm": 3.1237564086914062,
      "learning_rate": 2.8976520417742794e-07,
      "loss": 6.0517,
      "step": 9760
    },
    {
      "epoch": 10.087674058793192,
      "grad_norm": 3.751339912414551,
      "learning_rate": 2.4352087070443895e-07,
      "loss": 5.9626,
      "step": 9780
    },
    {
      "epoch": 10.108303249097473,
      "grad_norm": 6.093374729156494,
      "learning_rate": 2.012853002380466e-07,
      "loss": 5.8824,
      "step": 9800
    },
    {
      "epoch": 10.128932439401753,
      "grad_norm": 4.514645099639893,
      "learning_rate": 1.630618955992702e-07,
      "loss": 5.9588,
      "step": 9820
    },
    {
      "epoch": 10.149561629706033,
      "grad_norm": 4.418097496032715,
      "learning_rate": 1.2885373635829755e-07,
      "loss": 5.8572,
      "step": 9840
    },
    {
      "epoch": 10.170190820010315,
      "grad_norm": 4.213253021240234,
      "learning_rate": 9.866357858642205e-08,
      "loss": 5.9118,
      "step": 9860
    },
    {
      "epoch": 10.190820010314596,
      "grad_norm": 4.296292781829834,
      "learning_rate": 7.249385463395375e-08,
      "loss": 5.9692,
      "step": 9880
    },
    {
      "epoch": 10.211449200618876,
      "grad_norm": 3.8961002826690674,
      "learning_rate": 5.0346672934270534e-08,
      "loss": 5.9945,
      "step": 9900
    },
    {
      "epoch": 10.232078390923157,
      "grad_norm": 4.038491249084473,
      "learning_rate": 3.2223817833931805e-08,
      "loss": 5.9887,
      "step": 9920
    },
    {
      "epoch": 10.252707581227437,
      "grad_norm": 3.186933994293213,
      "learning_rate": 1.8126749448943437e-08,
      "loss": 5.9077,
      "step": 9940
    },
    {
      "epoch": 10.273336771531717,
      "grad_norm": 3.7681210041046143,
      "learning_rate": 8.056603547090813e-09,
      "loss": 5.9046,
      "step": 9960
    },
    {
      "epoch": 10.293965961835998,
      "grad_norm": 3.168226480484009,
      "learning_rate": 2.0141914564453244e-09,
      "loss": 6.0109,
      "step": 9980
    },
    {
      "epoch": 10.314595152140278,
      "grad_norm": 4.113708972930908,
      "learning_rate": 0.0,
      "loss": 6.0026,
      "step": 10000
    },
    {
      "epoch": 10.314595152140278,
      "eval_accuracy": 0.008525491542018992,
      "eval_loss": 6.127485752105713,
      "eval_runtime": 70.8386,
      "eval_samples_per_second": 23.448,
      "eval_steps_per_second": 5.873,
      "step": 10000
    }
  ],
  "logging_steps": 20,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 11,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1411682797274112.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
